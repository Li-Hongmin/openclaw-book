# âœï¸ ç¬¬9ç« ï¼šå†…å®¹ç”Ÿäº§è‡ªåŠ¨åŒ–

å†…å®¹åˆ›ä½œæ˜¯äº’è”ç½‘æ—¶ä»£æœ€æœ‰ä»·å€¼çš„æŠ€èƒ½ä¹‹ä¸€ï¼Œä½†ä¹Ÿæ˜¯æœ€è€—æ—¶ã€æœ€éœ€è¦åˆ›é€ åŠ›çš„å·¥ä½œã€‚æ— è®ºä½ æ˜¯YouTubeåˆ›ä½œè€…ã€åšå®¢ä½œè€…ã€è¿˜æ˜¯äº§å“ç»ç†ï¼Œéƒ½ä¼šé¢ä¸´ç›¸ä¼¼çš„æŒ‘æˆ˜ï¼šé€‰é¢˜å›°éš¾ã€ç ”ç©¶è€—æ—¶ã€é‡å¤æ€§åŠ³åŠ¨ç¹é‡ã€‚OpenClawä¸ºä½ æä¾›äº†ä¸€å¥—å®Œæ•´çš„å†…å®¹ç”Ÿäº§è‡ªåŠ¨åŒ–å·¥å…·é“¾ï¼Œè®©ä½ ä»ideaåˆ°å‘å¸ƒçš„æ¯ä¸ªç¯èŠ‚éƒ½èƒ½å¾—åˆ°AIçš„æ™ºèƒ½è¾…åŠ©ã€‚

æœ¬ç« å°†å¸¦ä½ æ„å»ºå±äºè‡ªå·±çš„å†…å®¹å·¥å‚ï¼Œä»Pipelineè®¾è®¡åˆ°å®æˆ˜æ¡ˆä¾‹ï¼Œä»å¸‚åœºè°ƒç ”åˆ°äº§å“éªŒè¯ï¼Œè®©ä½ çš„åˆ›æ„äº§å‡ºæ•ˆç‡æå‡10å€ä»¥ä¸Šã€‚

---

## ğŸš§ 9.1 å†…å®¹åˆ›ä½œçš„ç“¶é¢ˆ

åœ¨å¼€å§‹è‡ªåŠ¨åŒ–ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆç†è§£å†…å®¹åˆ›ä½œçš„çœŸæ­£ç—›ç‚¹åœ¨å“ªé‡Œã€‚

### 9.1.1 é€‰é¢˜å›°éš¾ï¼šçµæ„Ÿæ¯ç«­çš„ç„¦è™‘

æ¯ä¸ªå†…å®¹åˆ›ä½œè€…éƒ½ç»å†è¿‡è¿™æ ·çš„æ—¶åˆ»ï¼šç›¯ç€ç©ºç™½çš„æ–‡æ¡£ï¼Œä¸çŸ¥é“è¯¥å†™ä»€ä¹ˆã€‚ä½ å¯èƒ½ï¼š

- **ä¸çŸ¥é“è§‚ä¼—æƒ³çœ‹ä»€ä¹ˆ**ï¼šå‡­ç›´è§‰é€‰é¢˜ï¼Œå‘å¸ƒååå“å¹³å¹³
- **æ‰¾ä¸åˆ°æ–°é²œè§’åº¦**ï¼šåŒä¸€è¯é¢˜å·²ç»è¢«æ— æ•°äººå†™è¿‡ï¼Œå¦‚ä½•å·®å¼‚åŒ–ï¼Ÿ
- **é”™è¿‡çƒ­ç‚¹æ—¶æœº**ï¼šç­‰ä½ å‘ç°æŸä¸ªè¯é¢˜ç«äº†ï¼Œå·²ç»è¿‡äº†é»„é‡‘ä¼ æ’­æœŸ

ä¼ ç»Ÿçš„è§£å†³æ–¹æ¡ˆæ˜¯"å¤šçœ‹å¤šæƒ³"ï¼Œä½†è¿™ç§æ–¹æ³•æ•ˆç‡ä½ä¸‹ä¸”ç¼ºä¹ç³»ç»Ÿæ€§ã€‚AI Agentå¯ä»¥å¸®ä½ ï¼š

- **æŒç»­ç›‘æ§**ï¼š24/7è¿½è¸ªRedditã€Xï¼ˆTwitterï¼‰ã€YouTubeè¯„è®ºåŒºï¼Œå‘ç°çœŸå®ç”¨æˆ·ç—›ç‚¹
- **è¶‹åŠ¿åˆ†æ**ï¼šè¯†åˆ«æ­£åœ¨ä¸Šå‡çš„è¯é¢˜ï¼Œè€Œä¸æ˜¯å·²ç»è¿‡æ—¶çš„çƒ­ç‚¹
- **è§’åº¦å»ºè®®**ï¼šåŸºäºä½ çš„ä¸“é•¿å’Œå—ä¼—ç‰¹å¾ï¼Œæ¨èç‹¬ç‰¹çš„åˆ‡å…¥ç‚¹

ğŸ’¡ **é€‰é¢˜çµæ„Ÿæ¥æºå…¬å¼**
```
å¥½é€‰é¢˜ = ç”¨æˆ·ç—›ç‚¹ Ã— æ—¶æ•ˆæ€§ Ã— ä½ çš„ç‹¬ç‰¹è§†è§’
```
AI Agentæ“…é•¿å‰ä¸¤è€…ï¼ˆå¤§è§„æ¨¡æ•°æ®ç›‘æ§å’Œè¶‹åŠ¿åˆ†æï¼‰ï¼Œè€Œä½ çš„ç‹¬ç‰¹è§†è§’æ— æ³•è¢«æ›¿ä»£ã€‚ä¸¤è€…ç»“åˆï¼Œæ‰æ˜¯æœ€ä½³é€‰é¢˜ç­–ç•¥ã€‚

### 9.1.2 ç ”ç©¶è€—æ—¶ï¼šä¿¡æ¯è¿‡è½½çš„å›°å¢ƒ

é€‰å®šä¸»é¢˜åï¼Œç ”ç©¶é˜¶æ®µå¾€å¾€å æ®æ•´ä¸ªåˆ›ä½œæµç¨‹çš„50%ä»¥ä¸Šæ—¶é—´ã€‚ä½ éœ€è¦ï¼š

- **æŸ¥æ‰¾èµ„æ–™**ï¼šGoogleæœç´¢ã€æµè§ˆè®ºæ–‡ã€æŸ¥çœ‹ç«å“å†…å®¹
- **ç­›é€‰ä¿¡æ¯**ï¼šè¾¨åˆ«å“ªäº›ä¿¡æ¯å¯é ã€å“ªäº›ä¸ä¸»é¢˜ç›¸å…³
- **ç»„ç»‡ç»“æ„**ï¼šå°†ç¢ç‰‡åŒ–ä¿¡æ¯æ•´åˆæˆé€»è¾‘æ¸…æ™°çš„æ¡†æ¶

è¿™ä¸ªè¿‡ç¨‹ä¸ä»…è€—æ—¶ï¼Œè¿˜å®¹æ˜“é™·å…¥"ä¿¡æ¯é»‘æ´"â€”â€”ä½ å¼€å§‹æŸ¥Aèµ„æ–™ï¼Œç‚¹äº†Bé“¾æ¥ï¼Œçœ‹åˆ°Cè¯é¢˜ï¼Œæœ€åå¿˜äº†æœ€åˆè¦ç ”ç©¶ä»€ä¹ˆã€‚

OpenClawçš„è§£å†³æ–¹æ¡ˆæ˜¯**çŸ¥è¯†åº“+æ™ºèƒ½ç ”ç©¶Agent**ï¼š

```mermaid
graph LR
    A[ç ”ç©¶ä¸»é¢˜] --> B[æ™ºèƒ½æœç´¢]
    B --> C[çŸ¥è¯†åº“æŸ¥è¯¢]
    B --> D[Webæœç´¢]
    C --> E[ç»“æ„åŒ–ç¬”è®°]
    D --> E
    E --> F[ç ”ç©¶å¡ç‰‡è¾“å‡º]
```

ä½ çš„Agentå¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆè¿™äº›å·¥ä½œï¼š
1. ä»ä½ çš„ä¸ªäººçŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ç¬”è®°ï¼ˆç¬¬2ç« å·²æ„å»ºï¼‰
2. æ‰§è¡Œç²¾å‡†çš„Webæœç´¢ï¼Œè¿‡æ»¤ä½è´¨é‡å†…å®¹
3. æå–å…³é”®ä¿¡æ¯ï¼Œç”Ÿæˆç»“æ„åŒ–ç ”ç©¶å¡ç‰‡
4. æ ‡æ³¨æ¥æºï¼Œæ–¹ä¾¿åç»­æ ¸å®

### 9.1.3 é‡å¤æ€§å·¥ä½œï¼šåˆ›æ„è¢«çäº‹åæ²¡

å†…å®¹åˆ›ä½œä¸ä»…ä»…æ˜¯"å†™ä½œ"ï¼Œè¿˜åŒ…å«å¤§é‡é‡å¤æ€§åŠ³åŠ¨ï¼š

- **æ ¼å¼åŒ–**ï¼šè°ƒæ•´æ ‡é¢˜æ ·å¼ã€æ’å…¥ä»£ç å—ã€ä¼˜åŒ–æ’ç‰ˆ
- **é…å›¾**ï¼šå¯»æ‰¾åˆé€‚çš„å›¾ç‰‡ã€åˆ¶ä½œç¼©ç•¥å›¾ã€å‹ç¼©å°ºå¯¸
- **å‘å¸ƒæµç¨‹**ï¼šå¤åˆ¶ç²˜è´´åˆ°CMSã€è®¾ç½®SEOå…ƒæ•°æ®ã€ç¤¾äº¤åª’ä½“åˆ†å‘
- **æ•°æ®è¿½è¸ª**ï¼šè®°å½•å‘å¸ƒæ—¶é—´ã€è¿½è¸ªé˜…è¯»é‡ã€åˆ†æç”¨æˆ·åé¦ˆ

è¿™äº›å·¥ä½œæœ¬èº«ä¸éœ€è¦åˆ›é€ åŠ›ï¼Œå´å æ®äº†å¤§é‡æ—¶é—´å’Œæ³¨æ„åŠ›ã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼Œå®ƒä»¬æ‰“æ–­äº†åˆ›ä½œå¿ƒæµâ€”â€”å½“ä½ æ²‰æµ¸åœ¨å†™ä½œçŠ¶æ€ä¸­ï¼Œçªç„¶éœ€è¦å»æ‰¾ä¸€å¼ é…å›¾ï¼Œæ€è·¯å°±æ–­äº†ã€‚

**è‡ªåŠ¨åŒ–çš„é»„é‡‘æ³•åˆ™**ï¼šå°†"åˆ›æ„å†³ç­–"å’Œ"æ‰§è¡ŒåŠ¨ä½œ"åˆ†ç¦»ã€‚

- **ä½ è´Ÿè´£åˆ›æ„å†³ç­–**ï¼šé€‰é¢˜æ–¹å‘ã€æ ¸å¿ƒè§‚ç‚¹ã€å†…å®¹è´¨é‡æŠŠæ§
- **Agentè´Ÿè´£æ‰§è¡ŒåŠ¨ä½œ**ï¼šç ”ç©¶ã€èµ·è‰ã€æ ¼å¼åŒ–ã€å‘å¸ƒ

ğŸ”§ **è‡ªåŠ¨åŒ–ä¼˜å…ˆçº§åˆ¤æ–­**
é—®è‡ªå·±ä¸‰ä¸ªé—®é¢˜ï¼š
1. è¿™ä¸ªä»»åŠ¡æ˜¯å¦æœ‰æ˜ç¡®çš„è§„åˆ™æˆ–æµç¨‹ï¼Ÿï¼ˆæ˜¯ â†’ é€‚åˆè‡ªåŠ¨åŒ–ï¼‰
2. è¿™ä¸ªä»»åŠ¡æ˜¯å¦éœ€è¦æ¯æ¬¡éƒ½ä»å¤´æ€è€ƒï¼Ÿï¼ˆå¦ â†’ é€‚åˆè‡ªåŠ¨åŒ–ï¼‰
3. è¿™ä¸ªä»»åŠ¡æ˜¯å¦å¯ä»¥ç”¨å·¥å…·æˆ–APIå®ç°ï¼Ÿï¼ˆæ˜¯ â†’ é€‚åˆè‡ªåŠ¨åŒ–ï¼‰

å¦‚æœä¸‰ä¸ªé—®é¢˜éƒ½æ˜¯è‚¯å®šç­”æ¡ˆï¼Œé‚£è¿™ä¸ªä»»åŠ¡å°±åº”è¯¥è‡ªåŠ¨åŒ–ã€‚

### 9.1.4 è´¨é‡ä¸é€Ÿåº¦çš„å¹³è¡¡

æœ€åä¸€ä¸ªç—›ç‚¹æ˜¯ï¼šå¦‚ä½•åœ¨ä¿è¯è´¨é‡çš„å‰æä¸‹æé«˜äº§å‡ºé€Ÿåº¦ï¼Ÿ

è®¸å¤šåˆ›ä½œè€…é™·å…¥ä¸¤ç§æç«¯ï¼š
- **è¿½æ±‚å®Œç¾**ï¼šä¸€ç¯‡æ–‡ç« æ”¹10éï¼Œå‘å¸ƒå‘¨æœŸé•¿è¾¾æ•°å‘¨ï¼Œäº§å‡ºé‡ä½
- **è¿½æ±‚é€Ÿåº¦**ï¼šå¿«é€Ÿç”Ÿäº§å†…å®¹ï¼Œè´¨é‡å‚å·®ä¸é½ï¼Œç²‰ä¸æµå¤±

AIè¾…åŠ©åˆ›ä½œçš„ä»·å€¼åœ¨äºæ‰“ç ´è¿™ä¸ªå›°å¢ƒï¼š

**é€Ÿåº¦æå‡**ï¼š
- ç ”ç©¶æ—¶é—´ï¼š3å°æ—¶ â†’ 15åˆ†é’Ÿ
- åˆç¨¿ç”Ÿæˆï¼š2å°æ—¶ â†’ 10åˆ†é’Ÿ
- æ ¼å¼åŒ–å‘å¸ƒï¼š30åˆ†é’Ÿ â†’ è‡ªåŠ¨åŒ–

**è´¨é‡ä¿è¯**ï¼š
- äººå·¥ä¿ç•™æœ€ç»ˆå®¡æ ¸æƒï¼šAIç”Ÿæˆåˆç¨¿ï¼Œä½ è´Ÿè´£ç²¾ä¿®
- å¤šè½®è¿­ä»£æˆæœ¬é™ä½ï¼šæ”¹10éä¹Ÿä¸ä¼šè€—è´¹å¤ªå¤šæ—¶é—´
- ä¸“æ³¨æ ¸å¿ƒä»·å€¼ï¼šä½ çš„ç²¾åŠ›é›†ä¸­åœ¨åˆ›æ„å’Œæ´å¯Ÿï¼Œè€Œä¸æ˜¯æ ¼å¼è°ƒæ•´

ğŸ“š **æ¡ˆä¾‹ï¼šMatt Welshçš„å†…å®¹ç­–ç•¥**
å‰Googleå·¥ç¨‹å¸ˆMatt Welshåœ¨ä¸€æ¬¡é‡‡è®¿ä¸­æåˆ°ï¼šä»–ä½¿ç”¨AIç”Ÿæˆåšå®¢åˆç¨¿åï¼Œä¼šè¿›è¡Œ3-5è½®äººå·¥ä¿®æ”¹ã€‚æ¯æ¬¡ä¿®æ”¹éƒ½ä¸“æ³¨äºï¼š
1. å¢åŠ ä¸ªäººç»éªŒå’Œç‹¬ç‰¹è§‚ç‚¹
2. è°ƒæ•´è¯­æ°”ä½¿å…¶æ›´ç¬¦åˆä¸ªäººé£æ ¼
3. è¡¥å……AIå¯èƒ½é—æ¼çš„æŠ€æœ¯ç»†èŠ‚

æœ€ç»ˆå‘å¸ƒçš„å†…å®¹ï¼Œ80%çš„äº‹å®æ€§ä¿¡æ¯æ¥è‡ªAIç ”ç©¶ï¼Œä½†100%çš„è§‚ç‚¹å’Œæ´å¯Ÿæ¥è‡ªä»–æœ¬äººã€‚è¿™ç§åä½œæ–¹å¼è®©ä»–çš„äº§å‡ºé‡ç¿»å€ï¼ŒåŒæ—¶è´¨é‡ä¸é™åå‡ã€‚

---

## ğŸ­ 9.2 Pipelineè®¾è®¡ï¼šä»Ideaåˆ°å‘å¸ƒ

ç°åœ¨è®©æˆ‘ä»¬è®¾è®¡ä¸€ä¸ªé€šç”¨çš„å†…å®¹ç”Ÿäº§Pipelineã€‚æ— è®ºä½ çš„å†…å®¹ç±»å‹æ˜¯è§†é¢‘ã€åšå®¢ã€è¿˜æ˜¯ç¤¾äº¤åª’ä½“å¸–å­ï¼Œéƒ½å¯ä»¥å¥—ç”¨è¿™ä¸ªæ¡†æ¶ã€‚

### 9.2.1 Pipelineçš„å…­ä¸ªé˜¶æ®µ

```
é€‰é¢˜ â†’ ç ”ç©¶ â†’ èµ·è‰ â†’ ç¼–è¾‘ â†’ é…å›¾ â†’ å‘å¸ƒ
```

æ¯ä¸ªé˜¶æ®µéƒ½æœ‰æ˜ç¡®çš„è¾“å…¥å’Œè¾“å‡ºï¼š

| é˜¶æ®µ | è¾“å…¥ | è¾“å‡º | è‡ªåŠ¨åŒ–ç¨‹åº¦ |
|------|------|------|-----------|
| é€‰é¢˜ | ç”¨æˆ·ç—›ç‚¹ã€è¶‹åŠ¿æ•°æ® | é€‰é¢˜åˆ—è¡¨ + è§’åº¦å»ºè®® | 80% |
| ç ”ç©¶ | ä¸»é¢˜å…³é”®è¯ | ç»“æ„åŒ–ç ”ç©¶å¡ç‰‡ | 90% |
| èµ·è‰ | ç ”ç©¶å¡ç‰‡ + å¤§çº² | åˆç¨¿å†…å®¹ | 70% |
| ç¼–è¾‘ | åˆç¨¿ | ç²¾ä¿®ç‰ˆæœ¬ | 30% |
| é…å›¾ | å†…å®¹ä¸»é¢˜ | å›¾ç‰‡/ç¼©ç•¥å›¾ | 60% |
| å‘å¸ƒ | æœ€ç»ˆç¨¿ | å‘å¸ƒåˆ°å¹³å° | 95% |

æ³¨æ„"è‡ªåŠ¨åŒ–ç¨‹åº¦"åˆ—ï¼šè¶Šæ¥è¿‘åˆ›æ„æ ¸å¿ƒçš„ç¯èŠ‚ï¼ˆå¦‚ç¼–è¾‘ï¼‰ï¼Œäººå·¥ä»‹å…¥æ¯”ä¾‹è¶Šé«˜ã€‚è¿™æ˜¯è®¾è®¡Pipelineçš„å…³é”®åŸåˆ™ã€‚

### 9.2.2 é˜¶æ®µ1ï¼šé€‰é¢˜ä¾¦å¯Ÿ

**ç›®æ ‡**ï¼šç”Ÿæˆ10-20ä¸ªé«˜è´¨é‡é€‰é¢˜ï¼Œæ¯å‘¨åˆ·æ–°ã€‚

**è‡ªåŠ¨åŒ–å®ç°**ï¼š

```yaml
# skill_content_radar.yaml
name: content-radar
description: æŒç»­ç›‘æ§Reddit/X/YouTubeè¯„è®ºï¼Œå‘ç°å†…å®¹é€‰é¢˜

triggers:
  - cron: "0 9 * * *"  # æ¯å¤©æ—©ä¸Š9ç‚¹

steps:
  - name: æŠ“å–Redditç—›ç‚¹
    action: web_search
    params:
      sources: [reddit]
      subreddits: [{{ config.target_subreddits }}]
      time_range: "last_24h"
      keywords: ["how to", "struggling with", "can't figure out"]
    
  - name: åˆ†æè¶‹åŠ¿
    action: analyze_trends
    params:
      min_upvotes: 50
      sentiment: "negative"  # ç—›ç‚¹å¾€å¾€è¡¨è¾¾ä¸ºè´Ÿé¢æƒ…ç»ª
    
  - name: ç”Ÿæˆé€‰é¢˜å»ºè®®
    action: llm_generate
    prompt: |
      åŸºäºä»¥ä¸‹ç”¨æˆ·ç—›ç‚¹ï¼Œç”Ÿæˆ5ä¸ªè§†é¢‘/æ–‡ç« é€‰é¢˜ï¼š
      {{ step.reddit_posts }}
      
      è¦æ±‚ï¼š
      1. é’ˆå¯¹æ€§å¼ºï¼ˆè§£å†³å…·ä½“é—®é¢˜ï¼‰
      2. æ ‡é¢˜å¸å¼•äºº
      3. åŒ¹é…æˆ‘çš„ä¸“é•¿é¢†åŸŸï¼š{{ config.expertise }}
    
  - name: ä¿å­˜åˆ°é€‰é¢˜åº“
    action: database_insert
    table: content_ideas
```

**è¾“å‡ºç¤ºä¾‹**ï¼š

```markdown
## æœ¬å‘¨é€‰é¢˜å»ºè®®ï¼ˆ2026-02-20ï¼‰

### é€‰é¢˜1ï¼šä¸ºä»€ä¹ˆä½ çš„Dockerå®¹å™¨æ€»æ˜¯"æ‚„æ‚„"é‡å¯ï¼Ÿ
- **æ¥æº**ï¼šr/dockerï¼Œ87 upvotes
- **ç—›ç‚¹**ï¼š"å®¹å™¨è¿è¡Œå‡ å¤©åè«åå…¶å¦™é‡å¯ï¼Œæ—¥å¿—é‡Œæ‰¾ä¸åˆ°åŸå› "
- **è§’åº¦**ï¼šæ·±å…¥OOMKilleræœºåˆ¶ + å®æˆ˜debugå·¥å…·
- **é¢„è®¡çƒ­åº¦**ï¼šâ­â­â­â­

### é€‰é¢˜2ï¼šKubernetesç½‘ç»œä¸ºä½•è¿™ä¹ˆéš¾ï¼Ÿç»™åˆå­¦è€…çš„å¯è§†åŒ–æŒ‡å—
- **æ¥æº**ï¼šr/kubernetesï¼Œ134 upvotes
- **ç—›ç‚¹**ï¼š"æ¦‚å¿µå¤ªå¤šï¼Œæ–‡æ¡£è®²ä¸æ¸…æ¥š"
- **è§’åº¦**ï¼šç”¨åŠ¨ç”»å›¾è§£Pod/Service/Ingresså…³ç³»
- **é¢„è®¡çƒ­åº¦**ï¼šâ­â­â­â­â­
```

ğŸ”§ **é€‰é¢˜åº“ç»´æŠ¤æŠ€å·§**
- å°†é€‰é¢˜ä¿å­˜åˆ°æ•°æ®åº“æˆ–Notionï¼Œè€Œä¸æ˜¯èŠå¤©è®°å½•
- æ ‡è®°çŠ¶æ€ï¼š`å¾…ç ”ç©¶`ã€`ç ”ç©¶ä¸­`ã€`å·²å‘å¸ƒ`
- å®šæœŸæ¸…ç†è¿‡æ—¶é€‰é¢˜ï¼ˆ3ä¸ªæœˆæœªä½¿ç”¨çš„ï¼‰
- è®°å½•é€‰é¢˜æ¥æºï¼Œæ–¹ä¾¿åç»­å½’å› 

### 9.2.3 é˜¶æ®µ2ï¼šæ™ºèƒ½ç ”ç©¶

**ç›®æ ‡**ï¼šå°†é€‰é¢˜è½¬åŒ–ä¸ºç»“æ„åŒ–ç ”ç©¶å¡ç‰‡ï¼ŒåŒ…å«äº‹å®ã€æ¡ˆä¾‹ã€æ•°æ®ã€‚

**è‡ªåŠ¨åŒ–å®ç°**ï¼š

```python
# research_agent.py
async def research_topic(topic: str, depth: str = "standard"):
    """
    æ·±åº¦ç ”ç©¶ä¸€ä¸ªä¸»é¢˜
    
    Args:
        topic: ç ”ç©¶ä¸»é¢˜
        depth: standard(15åˆ†é’Ÿ) | deep(30åˆ†é’Ÿ)
    """
    
    # 1. æŸ¥è¯¢ä¸ªäººçŸ¥è¯†åº“
    kb_results = await knowledge_base.search(topic, limit=10)
    
    # 2. Webæœç´¢
    web_results = await brave_search(
        query=topic,
        count=20,
        freshness="pw"  # past week
    )
    
    # 3. æå–å…³é”®ä¿¡æ¯
    research_card = await llm.generate(
        prompt=f"""
        ç ”ç©¶ä¸»é¢˜ï¼š{topic}
        
        çŸ¥è¯†åº“ç›¸å…³ç¬”è®°ï¼š
        {kb_results}
        
        æœ€æ–°Webèµ„æ–™ï¼š
        {web_results}
        
        è¯·ç”Ÿæˆç»“æ„åŒ–ç ”ç©¶å¡ç‰‡ï¼š
        
        ## æ ¸å¿ƒæ¦‚å¿µ
        ï¼ˆå®šä¹‰ã€èƒŒæ™¯ï¼‰
        
        ## å…³é”®æ•°æ®
        ï¼ˆç»Ÿè®¡æ•°å­—ã€è°ƒç ”ç»“æœï¼‰
        
        ## çœŸå®æ¡ˆä¾‹
        ï¼ˆæˆåŠŸæ¡ˆä¾‹2ä¸ª + å¤±è´¥æ¡ˆä¾‹1ä¸ªï¼‰
        
        ## äº‰è®®ç‚¹
        ï¼ˆä¸åŒè§‚ç‚¹ã€æœªè§£å†³çš„é—®é¢˜ï¼‰
        
        ## å‚è€ƒæ¥æº
        ï¼ˆæ ‡æ³¨URLå’Œå‘å¸ƒæ—¥æœŸï¼‰
        """,
        model="claude-sonnet-4"
    )
    
    # 4. ä¿å­˜ç ”ç©¶å¡ç‰‡
    await save_markdown(
        path=f"research/{sanitize_filename(topic)}.md",
        content=research_card
    )
    
    return research_card
```

**è¾“å‡ºç¤ºä¾‹**ï¼ˆKubernetesç½‘ç»œä¸»é¢˜ï¼‰ï¼š

```markdown
# ç ”ç©¶å¡ç‰‡ï¼šKubernetesç½‘ç»œæ¨¡å‹

## æ ¸å¿ƒæ¦‚å¿µ
- **CNIï¼ˆContainer Network Interfaceï¼‰**ï¼šK8sç½‘ç»œæ’ä»¶æ ‡å‡†æ¥å£
- **Podç½‘ç»œ**ï¼šæ¯ä¸ªPodæœ‰ç‹¬ç«‹IPï¼ŒåŒä¸€Podå†…å®¹å™¨å…±äº«ç½‘ç»œå‘½åç©ºé—´
- **ServiceæŠ½è±¡**ï¼šé€šè¿‡ClusterIP/NodePort/LoadBalanceræš´éœ²åº”ç”¨

## å…³é”®æ•°æ®
- Flannelä½¿ç”¨ç‡ï¼š42%ï¼ˆCNCFè°ƒç ”2025ï¼‰
- Calicoä½¿ç”¨ç‡ï¼š38%
- å¹³å‡ç½‘ç»œå»¶è¿Ÿï¼šPodå†…é€šä¿¡ <0.1msï¼Œè·¨Nodeé€šä¿¡ 0.5-2ms

## çœŸå®æ¡ˆä¾‹
**æˆåŠŸæ¡ˆä¾‹1ï¼šShopifyçš„ç½‘ç»œä¼˜åŒ–**
- é—®é¢˜ï¼šè·¨AZé€šä¿¡å»¶è¿Ÿå¯¼è‡´ç»“è´¦é¡µé¢æ…¢
- æ–¹æ¡ˆï¼šä½¿ç”¨Cilium + eBPFç»•è¿‡iptables
- æ•ˆæœï¼šå»¶è¿Ÿé™ä½60%

**å¤±è´¥æ¡ˆä¾‹ï¼šåˆ›ä¸šå…¬å¸çš„ç½‘ç»œå™©æ¢¦**
- é—®é¢˜ï¼šç›´æ¥ç”¨Calicoé»˜è®¤é…ç½®ï¼ŒIPåœ°å€æ± å†²çª
- åæœï¼šçº¿ä¸ŠæœåŠ¡ä¸­æ–­4å°æ—¶
- æ•™è®­ï¼šå¿…é¡»æå‰è§„åˆ’IP CIDR

## äº‰è®®ç‚¹
- **Overlay vs Underlay**ï¼šæ€§èƒ½ä¸å…¼å®¹æ€§çš„æƒè¡¡
- **NetworkPolicyçš„å¿…è¦æ€§**ï¼šå°å›¢é˜Ÿæ˜¯å¦éœ€è¦å¤æ‚çš„ç½‘ç»œéš”ç¦»ï¼Ÿ

## å‚è€ƒæ¥æº
- [Kubernetesç½‘ç»œæ¨¡å‹è¯¦è§£](https://kubernetes.io/docs/concepts/cluster-administration/networking/) - 2025-11
- [CNCFç½‘ç»œæ’ä»¶è°ƒç ”æŠ¥å‘Š](https://www.cncf.io/reports/network-survey-2025/) - 2025-09
```

ğŸ’¡ **ç ”ç©¶æ·±åº¦çš„åˆ¤æ–­æ ‡å‡†**
- **Standardç ”ç©¶**ï¼ˆ15åˆ†é’Ÿï¼‰ï¼šé€‚ç”¨äºæ•™ç¨‹ç±»ã€æ“ä½œæŒ‡å—ç±»å†…å®¹ï¼Œéœ€è¦å‡†ç¡®æ€§ä½†ä¸éœ€è¦æ·±åº¦æ´å¯Ÿ
- **Deepç ”ç©¶**ï¼ˆ30-60åˆ†é’Ÿï¼‰ï¼šé€‚ç”¨äºåˆ†æç±»ã€è§‚ç‚¹ç±»å†…å®¹ï¼Œéœ€è¦å¼•ç”¨æƒå¨æ•°æ®å’Œå¤šè§’åº¦å¯¹æ¯”
- **Expertç ”ç©¶**ï¼ˆåŠå¤©ï¼‰ï¼šé€‚ç”¨äºç™½çš®ä¹¦ã€æŠ€æœ¯æŠ¥å‘Šï¼Œéœ€è¦åŸåˆ›æ€§ç ”ç©¶å’Œå®éªŒéªŒè¯

### 9.2.4 é˜¶æ®µ3ï¼šåˆç¨¿ç”Ÿæˆ

**ç›®æ ‡**ï¼šåŸºäºç ”ç©¶å¡ç‰‡ï¼Œç”Ÿæˆ80%å®Œæˆåº¦çš„åˆç¨¿ã€‚

è¿™æ˜¯Pipelineä¸­æœ€å¾®å¦™çš„ç¯èŠ‚ã€‚ç”Ÿæˆçš„åˆç¨¿éœ€è¦ï¼š
- **ç»“æ„æ¸…æ™°**ï¼šæœ‰æ˜ç¡®çš„èµ·æ‰¿è½¬åˆ
- **ä¿¡æ¯å‡†ç¡®**ï¼šæ¥è‡ªç ”ç©¶å¡ç‰‡ï¼Œè€Œä¸æ˜¯AIç¼–é€ 
- **é£æ ¼ç»Ÿä¸€**ï¼šç¬¦åˆä½ çš„ä¸ªäººé£æ ¼ï¼ˆéœ€è¦æä¾›é£æ ¼æŒ‡å—ï¼‰

**é£æ ¼æŒ‡å—ç¤ºä¾‹**ï¼š

```markdown
# æˆ‘çš„å†™ä½œé£æ ¼æŒ‡å—

## è¯­æ°”
- å‹å¥½ä½†ä¸“ä¸šï¼Œåƒå’Œæœ‹å‹èŠå¤©ä¸€æ ·è§£é‡ŠæŠ€æœ¯
- é¿å…è¿‡åº¦ä½¿ç”¨æœ¯è¯­ï¼Œå¿…è¦æ—¶æä¾›ç±»æ¯”è§£é‡Š
- å¯ä»¥é€‚å½“è‡ªå˜²ï¼Œä½†ä¸è¦è´¬ä½è¯»è€…

## ç»“æ„
- å¼€å¤´å¿…é¡»æœ‰hookï¼šæå‡ºé—®é¢˜æˆ–è®²è¿°åœºæ™¯
- æ¯ä¸ªç« èŠ‚æœ‰æ˜ç¡®çš„ä¸­å¿ƒè®ºç‚¹
- ä½¿ç”¨å¤§é‡ä»£ç ç¤ºä¾‹å’Œå¯è§†åŒ–å›¾è¡¨
- ç»“å°¾å¿…é¡»æœ‰è¡ŒåŠ¨å‘¼åï¼ˆCTAï¼‰

## ç¦å¿Œ
- ä¸è¦è¯´"åœ¨å½“ä»Šå¿«é€Ÿå‘å±•çš„æŠ€æœ¯ä¸–ç•Œä¸­"è¿™ç§å¥—è¯
- ä¸è¦è¿‡åº¦ä½¿ç”¨emojiï¼ˆæ¯æ®µæœ€å¤š1ä¸ªï¼‰
- ä¸è¦å†™"ä¼—æ‰€å‘¨çŸ¥"ã€"æ˜¾è€Œæ˜“è§"ï¼ˆå¯¹è¯»è€…ä¸å‹å¥½ï¼‰

## ç¤ºä¾‹æ®µè½
ã€ç²˜è´´ä½ ä¹‹å‰æ»¡æ„çš„3-5æ®µæ–‡å­—ã€‘
```

**ç”Ÿæˆåˆç¨¿çš„Promptå·¥ç¨‹**ï¼š

```python
async def generate_draft(research_card: str, style_guide: str, outline: str):
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯å†…å®¹ä½œè€…ã€‚è¯·åŸºäºç ”ç©¶å¡ç‰‡ç”Ÿæˆæ–‡ç« åˆç¨¿ã€‚

# ç ”ç©¶å¡ç‰‡
{research_card}

# å¤§çº²
{outline}

# é£æ ¼æŒ‡å—
{style_guide}

# è¦æ±‚
1. ä¸¥æ ¼åŸºäºç ”ç©¶å¡ç‰‡çš„äº‹å®ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœç ”ç©¶å¡ç‰‡ä¸­æŸä¸ªéƒ¨åˆ†ä¿¡æ¯ä¸è¶³ï¼Œæ ‡è®°ä¸º [éœ€è¡¥å……ï¼šXXX]
3. éµå¾ªé£æ ¼æŒ‡å—çš„è¯­æ°”å’Œç»“æ„
4. æ¯ä¸ªæŠ€æœ¯æ¦‚å¿µé¦–æ¬¡å‡ºç°æ—¶æä¾›ç®€çŸ­è§£é‡Š
5. åœ¨åˆé€‚ä½ç½®æ’å…¥ `[é…å›¾å»ºè®®ï¼šXXX]` æç¤º

# è¾“å‡ºæ ¼å¼
Markdownæ ¼å¼ï¼ŒåŒ…å«æ ‡é¢˜ã€æ­£æ–‡ã€ä»£ç å—ã€‚

å¼€å§‹ç”Ÿæˆï¼š
"""
    
    draft = await llm.generate(prompt, model="claude-sonnet-4", max_tokens=8000)
    return draft
```

**ç”Ÿæˆçš„åˆç¨¿ç¤ºä¾‹**ï¼ˆèŠ‚é€‰ï¼‰ï¼š

```markdown
# Kubernetesç½‘ç»œä¸ºä½•è¿™ä¹ˆéš¾ï¼Ÿç»™åˆå­¦è€…çš„å¯è§†åŒ–æŒ‡å—

å¦‚æœä½ åˆšå¼€å§‹å­¦Kubernetesï¼Œä¸€å®šè¢«å„ç§ç½‘ç»œæ¦‚å¿µææ™•è¿‡ï¼šPodç½‘ç»œã€Serviceã€Ingressã€CNIã€Overlay...æ¯ä¸ªè¯å•ç‹¬çœ‹éƒ½è®¤è¯†ï¼Œä½†æ”¾åœ¨ä¸€èµ·å°±ä¸çŸ¥é“åœ¨è¯´ä»€ä¹ˆã€‚

æˆ‘ç¬¬ä¸€æ¬¡éƒ¨ç½²K8sé›†ç¾¤æ—¶ï¼ŒèŠ±äº†æ•´æ•´ä¸¤å¤©æ‰è®©ä¸¤ä¸ªPodäº’ç›¸é€šä¿¡æˆåŠŸã€‚æ–‡æ¡£è®²å¾—å¾ˆè¯¦ç»†ï¼Œä½†å°±æ˜¯çœ‹ä¸æ‡‚"ä¸ºä»€ä¹ˆ"è¦è¿™ä¹ˆè®¾è®¡ã€‚

ä»Šå¤©æˆ‘ä»¬ç”¨å¯è§†åŒ–çš„æ–¹å¼ï¼ŒæŠŠK8sç½‘ç»œä»åº•å±‚åˆ°ä¸Šå±‚æ‹ä¸€éã€‚è¯»å®Œè¿™ç¯‡æ–‡ç« ï¼Œä½ ä¼šç†è§£æ¯ä¸ªç»„ä»¶çš„ä½œç”¨ï¼Œä»¥åŠå®ƒä»¬ä¹‹é—´å¦‚ä½•é…åˆã€‚

[é…å›¾å»ºè®®ï¼šK8sç½‘ç»œå…¨æ™¯å›¾ï¼Œå±•ç¤ºPod/Service/Ingresså±‚æ¬¡å…³ç³»]

## ç¬¬ä¸€å±‚ï¼šPodç½‘ç»œçš„ç§˜å¯†

åœ¨ä¼ ç»Ÿçš„Dockeréƒ¨ç½²ä¸­ï¼Œå®¹å™¨çš„ç½‘ç»œæ¨¡å¼é€šå¸¸æ˜¯æ¡¥æ¥ï¼ˆbridgeï¼‰ã€‚ä½†åœ¨K8sé‡Œï¼Œæ¯ä¸ªPodæœ‰è‡ªå·±çš„IPåœ°å€â€”â€”è¿™å¬èµ·æ¥å¾ˆç®€å•ï¼Œä½†å®ç°èµ·æ¥æœ‰å·¨å¤§çš„å·¥ç¨‹æŒ‘æˆ˜ã€‚

**é—®é¢˜ï¼šå¦‚ä½•è®©ä¸åŒNodeä¸Šçš„Podäº’ç›¸é€šä¿¡ï¼Ÿ**

å‡è®¾ä½ æœ‰ä¸¤ä¸ªNodeï¼š
- Node1 ä¸Šè¿è¡Œ Pod-A (IP: 10.244.1.5)
- Node2 ä¸Šè¿è¡Œ Pod-B (IP: 10.244.2.8)

Pod-A æƒ³è®¿é—® Pod-Bï¼Œæ•°æ®åŒ…å¦‚ä½•è·¯ç”±ï¼Ÿ

[éœ€è¡¥å……ï¼šCNIæ’ä»¶çš„å…·ä½“å®ç°å¯¹æ¯”]

## ç¬¬äºŒå±‚ï¼šServiceçš„è´Ÿè½½å‡è¡¡é­”æ³•

ï¼ˆç»§ç»­ç”Ÿæˆ...ï¼‰
```

æ³¨æ„ç”Ÿæˆçš„åˆç¨¿ä¸­åŒ…å«ï¼š
- `[é…å›¾å»ºè®®]`ï¼šæç¤ºä¸‹ä¸€é˜¶æ®µéœ€è¦åˆ¶ä½œçš„å›¾ç‰‡
- `[éœ€è¡¥å……]`ï¼šæ ‡è®°ä¿¡æ¯ä¸è¶³çš„éƒ¨åˆ†ï¼Œæ–¹ä¾¿ä½ äººå·¥è¡¥å……

### 9.2.5 é˜¶æ®µ4ï¼šäººå·¥ç¼–è¾‘

è¿™æ˜¯Pipelineä¸­æœ€é‡è¦çš„äººå·¥ç¯èŠ‚ã€‚ä½ éœ€è¦ï¼š

1. **å¡«è¡¥ç©ºç™½**ï¼šè¡¥å……AIæ ‡è®°çš„ `[éœ€è¡¥å……]` éƒ¨åˆ†
2. **å¢åŠ ä¸ªæ€§**ï¼šåŠ å…¥ä½ çš„ä¸ªäººç»éªŒã€ç‹¬ç‰¹è§‚ç‚¹ã€å¹½é»˜è¡¨è¾¾
3. **è°ƒæ•´èŠ‚å¥**ï¼šåˆ é™¤å†—ä½™å†…å®¹ï¼Œè°ƒæ•´æ®µè½é•¿åº¦
4. **è´¨é‡æŠŠæ§**ï¼šæ£€æŸ¥äº‹å®å‡†ç¡®æ€§ï¼Œç¡®ä¿ä»£ç å¯è¿è¡Œ

ğŸ”§ **ç¼–è¾‘æ•ˆç‡æå‡æŠ€å·§**
- ä½¿ç”¨AIè¾…åŠ©ç¼–è¾‘ï¼š
  - "è¿™æ®µè¯å¤ªå­¦æœ¯äº†ï¼Œæ”¹æˆæ›´å£è¯­åŒ–çš„ç‰ˆæœ¬"
  - "ç»™è¿™ä¸ªæŠ€æœ¯æ¦‚å¿µåŠ ä¸€ä¸ªç±»æ¯”è§£é‡Š"
  - "è¿™ä¸ªæ®µè½ä¿¡æ¯å¯†åº¦å¤ªé«˜ï¼Œæ‹†åˆ†æˆä¸¤æ®µ"
- ä¿ç•™åˆç¨¿ç‰ˆæœ¬ï¼šä½¿ç”¨Gitæˆ–Notionç‰ˆæœ¬å†å²ï¼Œæ–¹ä¾¿å›é€€
- æ‰¹é‡å¤„ç†åŒç±»ä¿®æ”¹ï¼šå…ˆå¤„ç†æ‰€æœ‰ `[éœ€è¡¥å……]`ï¼Œå†ç»Ÿä¸€è°ƒæ•´è¯­æ°”

### 9.2.6 é˜¶æ®µ5ï¼šé…å›¾ç”Ÿæˆ

**è‡ªåŠ¨åŒ–æ–¹æ¡ˆ**ï¼š

```python
async def generate_images(article_md: str):
    """
    è§£ææ–‡ç« ä¸­çš„ [é…å›¾å»ºè®®] æ ‡è®°ï¼Œç”Ÿæˆé…å›¾
    """
    image_requests = re.findall(r'\[é…å›¾å»ºè®®ï¼š(.*?)\]', article_md)
    
    images = []
    for req in image_requests:
        # æ–¹æ¡ˆ1ï¼šAIç”Ÿæˆå›¾ï¼ˆDALL-E / Midjourneyï¼‰
        img_url = await dalle.generate(
            prompt=f"Technical diagram: {req}, flat design, clean background",
            size="1024x1024"
        )
        
        # æ–¹æ¡ˆ2ï¼šä»å›¾åº“æœç´¢ï¼ˆUnsplash / Pexelsï¼‰
        # img_url = await unsplash.search(req)
        
        images.append({
            "description": req,
            "url": img_url
        })
    
    return images
```

**ç¼©ç•¥å›¾ç‰¹æ®Šå¤„ç†**ï¼š

YouTubeã€åšå®¢ç­‰å¹³å°çš„ç¼©ç•¥å›¾å¯¹ç‚¹å‡»ç‡å½±å“å·¨å¤§ã€‚ä½ å¯ä»¥ï¼š

1. **æ¨¡æ¿åŒ–ç”Ÿæˆ**ï¼šå‡†å¤‡å‡ ä¸ªFigma/Canvaæ¨¡æ¿ï¼ŒAgentè‡ªåŠ¨æ›¿æ¢æ–‡å­—
2. **AIç”Ÿæˆ + äººå·¥ç­›é€‰**ï¼šç”Ÿæˆ3ä¸ªå€™é€‰ï¼Œä½ é€‰æ‹©æœ€ä½³
3. **A/Bæµ‹è¯•**ï¼šå‘å¸ƒæ—¶ä½¿ç”¨ä¸åŒç¼©ç•¥å›¾ï¼Œè¿½è¸ªç‚¹å‡»ç‡

```python
async def generate_thumbnail(title: str, style: str = "tech"):
    """
    ç”ŸæˆYouTubeé£æ ¼ç¼©ç•¥å›¾
    """
    prompt = f"""
    Create a YouTube thumbnail for video titled: "{title}"
    
    Style: {style}
    - Bold text overlay
    - High contrast colors
    - Eye-catching composition
    - 1280x720 resolution
    """
    
    # ç”Ÿæˆ3ä¸ªå€™é€‰
    candidates = []
    for i in range(3):
        img = await dalle.generate(prompt, size="1280x720")
        candidates.append(img)
    
    # è¿”å›å€™é€‰è®©ç”¨æˆ·é€‰æ‹©
    return candidates
```

### 9.2.7 é˜¶æ®µ6ï¼šè‡ªåŠ¨å‘å¸ƒ

**ç›®æ ‡**ï¼šä¸€é”®å‘å¸ƒåˆ°å¤šä¸ªå¹³å°ï¼Œè‡ªåŠ¨å¤„ç†æ ¼å¼å·®å¼‚ã€‚

```python
async def publish_content(article: Article, platforms: List[str]):
    """
    å‘å¸ƒå†…å®¹åˆ°å¤šä¸ªå¹³å°
    """
    results = {}
    
    for platform in platforms:
        if platform == "wordpress":
            # WordPress API
            result = await wordpress_client.create_post(
                title=article.title,
                content=article.markdown_to_html(),
                categories=article.tags,
                featured_image=article.thumbnail
            )
        
        elif platform == "medium":
            # Medium API
            result = await medium_client.create_post(
                title=article.title,
                content=article.markdown,
                tags=article.tags[:5],  # Mediumæœ€å¤š5ä¸ªæ ‡ç­¾
                publish_status="draft"  # å…ˆå‘è‰ç¨¿ï¼Œäººå·¥å®¡æ ¸åå‘å¸ƒ
            )
        
        elif platform == "dev.to":
            # Dev.to API
            result = await devto_client.create_article(
                title=article.title,
                body_markdown=article.markdown,
                tags=article.tags,
                published=False  # å…ˆä¿å­˜ä¸ºè‰ç¨¿
            )
        
        results[platform] = result
    
    return results
```

**å‘å¸ƒåè‡ªåŠ¨åŒ–**ï¼š

```yaml
# post_publish_automation.yaml
triggers:
  - on_publish

steps:
  - name: ç¤¾äº¤åª’ä½“åˆ†å‘
    actions:
      - twitter_post:
          text: "{{ article.title }} å·²å‘å¸ƒï¼{{ article.url }}"
      - linkedin_post:
          text: "{{ article.excerpt }}"
          url: "{{ article.url }}"
  
  - name: é€šçŸ¥è®¢é˜…è€…
    action: email_send
    template: new_article_notification
    recipients: "{{ subscribers }}"
  
  - name: è®°å½•Analytics
    action: database_insert
    table: published_articles
    data:
      title: "{{ article.title }}"
      url: "{{ article.url }}"
      publish_time: "{{ now }}"
```

ğŸ“š **å‘å¸ƒæ—¶æœºä¼˜åŒ–**
ä¸åŒå¹³å°æœ‰æœ€ä½³å‘å¸ƒæ—¶é—´ï¼š
- **ä¸ªäººåšå®¢**ï¼šå‘¨äºŒ-å‘¨å››ä¸Šåˆ10ç‚¹ï¼ˆå·¥ä½œåœºæ™¯é˜…è¯»ï¼‰
- **Reddit**ï¼šå‘¨ä¸€-å‘¨ä¸‰æ™šä¸Š8-10ç‚¹ï¼ˆä¸‹ç­åæµè§ˆï¼‰
- **YouTube**ï¼šå‘¨æœ«ä¸‹åˆ2-5ç‚¹ï¼ˆå¨±ä¹æ—¶é—´ï¼‰

Agentå¯ä»¥è‡ªåŠ¨é€‰æ‹©æœ€ä½³å‘å¸ƒæ—¶é—´ï¼Œæˆ–å°†å†…å®¹åŠ å…¥å‘å¸ƒé˜Ÿåˆ—ã€‚

---

## ğŸ’» 9.3 æ¡ˆä¾‹ç¾¤å®æˆ˜

ç†è®ºè®²å®Œäº†ï¼Œç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸‰ä¸ªå®Œæ•´çš„å®æˆ˜æ¡ˆä¾‹ï¼Œæ¶µç›–ä¸åŒçš„å†…å®¹ç±»å‹å’Œå¤æ‚åº¦ã€‚

### 9.3.1 æ¡ˆä¾‹1ï¼šYouTubeå†…å®¹ç®¡é“

**åœºæ™¯**ï¼šä½ è¿è¥ä¸€ä¸ªæŠ€æœ¯æ•™ç¨‹YouTubeé¢‘é“ï¼Œæ¯å‘¨å‘å¸ƒ1-2ä¸ªè§†é¢‘ã€‚

**ç›®æ ‡**ï¼šå°†è§†é¢‘åˆ¶ä½œæµç¨‹ä»"æ¯å‘¨20å°æ—¶"ç¼©çŸ­åˆ°"æ¯å‘¨8å°æ—¶"ã€‚

**å®Œæ•´Pipeline**ï¼š

```mermaid
graph TD
    A[é€‰é¢˜ä¾¦å¯ŸAgent] --> B[ç ”ç©¶Agent]
    B --> C[è„šæœ¬è‰ç¨¿Agent]
    C --> D[äººå·¥ç²¾ä¿®]
    D --> E[å½•åˆ¶è§†é¢‘]
    E --> F[ä¸Šä¼ å‘å¸ƒAgent]
    F --> G[åˆ†æåé¦ˆAgent]
    G --> A
```

**é˜¶æ®µ1ï¼šé€‰é¢˜ä¾¦å¯Ÿ**

åˆ©ç”¨ç¬¬8ç« çš„YouTubeé¢‘é“è¿½è¸ªå’Œç¬¬2ç« çš„çŸ¥è¯†åº“ï¼Œä½ çš„Agentå¯ä»¥ï¼š

```python
# youtube_topic_scout.py
async def scout_youtube_topics():
    """
    ä»YouTubeè¯„è®ºåŒºå’ŒRedditæŒ–æ˜è§†é¢‘é€‰é¢˜
    """
    
    # 1. è¿½è¸ªç«å“é¢‘é“çš„çƒ­é—¨è§†é¢‘
    hot_videos = await youtube_api.get_trending_videos(
        category="Science & Technology",
        region="US",
        time_range="week"
    )
    
    # 2. åˆ†æè¯„è®ºåŒºç—›ç‚¹
    pain_points = []
    for video in hot_videos:
        comments = await youtube_api.get_comments(video.id, max_results=100)
        
        # æå–é—®é¢˜å‹è¯„è®º
        questions = [c for c in comments if "how" in c.text.lower() or "why" in c.text.lower()]
        
        # èšç±»ç›¸ä¼¼é—®é¢˜
        clusters = await llm.cluster_texts(questions)
        pain_points.extend(clusters)
    
    # 3. ç”Ÿæˆè§†é¢‘é€‰é¢˜
    topics = await llm.generate(f"""
    åŸºäºä»¥ä¸‹YouTubeç”¨æˆ·ç—›ç‚¹ï¼Œç”Ÿæˆ5ä¸ªè§†é¢‘é€‰é¢˜ï¼š
    
    {pain_points}
    
    è¦æ±‚ï¼š
    - æ ‡é¢˜å¸å¼•äººï¼ŒåŒ…å«æ•°å­—æˆ–ç–‘é—®å¥
    - æ—¶é•¿æ§åˆ¶åœ¨10-15åˆ†é’Ÿ
    - éš¾åº¦é€‚ä¸­ï¼ˆåˆå­¦è€…èƒ½çœ‹æ‡‚ï¼Œæœ‰ç»éªŒè€…ä¹Ÿæœ‰æ”¶è·ï¼‰
    - æä¾›æ¸…æ™°çš„ä»·å€¼æ‰¿è¯ºï¼ˆ"çœ‹å®Œä½ å°†å­¦ä¼š..."ï¼‰
    """)
    
    return topics
```

**è¾“å‡ºç¤ºä¾‹**ï¼š

```markdown
## æœ¬å‘¨è§†é¢‘é€‰é¢˜

### é€‰é¢˜1ï¼šDocker Compose vs Kubernetesï¼š5åˆ†é’Ÿè®²æ¸…æ¥šé€‰æ‹©æ ‡å‡†
- **æ¥æº**ï¼šr/docker 67ä¸ªè¯„è®ºè®¨è®º "ä»€ä¹ˆæ—¶å€™è¯¥ç”¨K8s"
- **ä»·å€¼æ‰¿è¯º**ï¼šçœ‹å®Œä½ å°†è·å¾—ä¸€ä¸ªå†³ç­–æ ‘ï¼Œæ ¹æ®é¡¹ç›®è§„æ¨¡å¿«é€Ÿé€‰æ‹©
- **é¢„è®¡æ—¶é•¿**ï¼š12åˆ†é’Ÿ
- **éš¾åº¦**ï¼šâ­â­ï¼ˆéœ€äº†è§£DockeråŸºç¡€ï¼‰

### é€‰é¢˜2ï¼šæˆ‘èŠ±äº†ä¸€å‘¨ä¼˜åŒ–SQLæŸ¥è¯¢ï¼Œæ€»ç»“å‡ºè¿™5ä¸ªæŠ€å·§
- **æ¥æº**ï¼šYouTubeçƒ­é—¨è§†é¢‘ã€Šæ•°æ®åº“æ€§èƒ½ä¼˜åŒ–ã€‹è¯„è®ºåŒº
- **ä»·å€¼æ‰¿è¯º**ï¼šå³å­¦å³ç”¨çš„ä¼˜åŒ–æŠ€å·§ï¼Œä¸éœ€è¦DBAèƒŒæ™¯
- **é¢„è®¡æ—¶é•¿**ï¼š15åˆ†é’Ÿ
- **éš¾åº¦**ï¼šâ­â­â­ï¼ˆéœ€äº†è§£SQLè¯­æ³•ï¼‰
```

**é˜¶æ®µ2ï¼šç ”ç©¶å¡ç‰‡**

ï¼ˆä½¿ç”¨å‰é¢ä»‹ç»çš„ `research_agent.py`ï¼‰

**é˜¶æ®µ3ï¼šè„šæœ¬è‰ç¨¿**

è§†é¢‘è„šæœ¬ä¸æ–‡ç« çš„åŒºåˆ«ï¼š
- **å£è¯­åŒ–**ï¼šæ›´å¤šçŸ­å¥ï¼Œé¿å…å¤æ‚ä»å¥
- **è§†è§‰æç¤º**ï¼šæ ‡æ³¨éœ€è¦å±•ç¤ºçš„ç”»é¢
- **èŠ‚å¥æ§åˆ¶**ï¼šæ ‡è®°åœé¡¿ç‚¹ã€é‡ç‚¹å¼ºè°ƒ

```python
async def generate_video_script(research_card: str, topic: str):
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªYouTubeè§†é¢‘è„šæœ¬ä½œè€…ã€‚è¯·åŸºäºç ”ç©¶å¡ç‰‡ç”Ÿæˆè§†é¢‘è„šæœ¬ã€‚

# ç ”ç©¶å¡ç‰‡
{research_card}

# è§†é¢‘é€‰é¢˜
{topic}

# è„šæœ¬è¦æ±‚
1. **Hookï¼ˆå‰30ç§’ï¼‰**ï¼šæå‡ºé—®é¢˜æˆ–ç—›ç‚¹ï¼Œå¸å¼•è§‚ä¼—ç»§ç»­çœ‹
2. **ä¸»ä½“å†…å®¹**ï¼šåˆ†æˆ3-5ä¸ªç« èŠ‚ï¼Œæ¯ä¸ªç« èŠ‚2-3åˆ†é’Ÿ
3. **CTAï¼ˆæœ€å30ç§’ï¼‰**ï¼šå¼•å¯¼è®¢é˜…ã€è¯„è®ºã€ç‚¹èµ

# æ ¼å¼è¦æ±‚
- ä½¿ç”¨ `[ç”»é¢ï¼šXXX]` æ ‡æ³¨éœ€è¦å±•ç¤ºçš„å†…å®¹
- ä½¿ç”¨ `[æš‚åœ 2ç§’]` æ ‡æ³¨åœé¡¿ç‚¹
- ä½¿ç”¨ `**é‡ç‚¹**` æ ‡æ³¨éœ€è¦å¼ºè°ƒçš„è¯å¥

å¼€å§‹ç”Ÿæˆï¼š
"""
    
    script = await llm.generate(prompt, model="claude-sonnet-4")
    return script
```

**ç”Ÿæˆçš„è„šæœ¬ç¤ºä¾‹**ï¼ˆèŠ‚é€‰ï¼‰ï¼š

```markdown
# è§†é¢‘è„šæœ¬ï¼šDocker Compose vs Kubernetesï¼š5åˆ†é’Ÿè®²æ¸…æ¥šé€‰æ‹©æ ‡å‡†

[ç”»é¢ï¼šæ ‡é¢˜å¡ + èƒŒæ™¯éŸ³ä¹]

## Hookï¼ˆ0:00 - 0:30ï¼‰

ä½ æ˜¯ä¸æ˜¯ç»å¸¸å¬äººè¯´ "Kubernetesæ˜¯æœªæ¥"ï¼Œç„¶åä½ å°±è·Ÿé£æŠŠé¡¹ç›®è¿ç§»åˆ°K8sï¼Œç»“æœå‘ç°**æ¯”ç”¨Docker Composeå¤æ‚10å€**ï¼Ÿ

[ç”»é¢ï¼šå±å¹•å½•åˆ¶ï¼Œå±•ç¤ºK8så¤æ‚çš„YAMLé…ç½®]

ä»Šå¤©æˆ‘ä»¬ç”¨ä¸€ä¸ªå†³ç­–æ ‘ï¼Œ5åˆ†é’Ÿè®²æ¸…æ¥šï¼š**ä»€ä¹ˆæ—¶å€™è¯¥ç”¨K8sï¼Œä»€ä¹ˆæ—¶å€™Docker Composeå°±å¤Ÿäº†ã€‚**

[æš‚åœ 2ç§’]

[ç”»é¢ï¼šè½¬åœºåŠ¨ç”»]

## ç¬¬ä¸€ç« èŠ‚ï¼šå®ƒä»¬åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆ0:30 - 2:00ï¼‰

é¦–å…ˆæˆ‘ä»¬å¿«é€Ÿå›é¡¾ä¸€ä¸‹è¿™ä¸¤ä¸ªå·¥å…·çš„æœ¬è´¨ã€‚

**Docker Compose**ï¼šä¸€ä¸ªé…ç½®æ–‡ä»¶ç®¡ç†å¤šä¸ªå®¹å™¨ã€‚

[ç”»é¢ï¼šå±•ç¤ºç®€å•çš„docker-compose.yml]

ä½ å®šä¹‰æœåŠ¡ã€ç½‘ç»œã€å·ï¼Œç„¶åä¸€æ¡å‘½ä»¤ `docker-compose up`ï¼Œæ‰€æœ‰å®¹å™¨å°±è·‘èµ·æ¥äº†ã€‚**ç®€å•ã€ç›´æ¥ã€‚**

[æš‚åœ 1ç§’]

**Kubernetes**ï¼šä¸€ä¸ªå®¹å™¨ç¼–æ’å¹³å°ã€‚

[ç”»é¢ï¼šå±•ç¤ºK8sæ¶æ„å›¾]

å®ƒä¸ä»…ç®¡ç†å®¹å™¨ï¼Œè¿˜è´Ÿè´£è°ƒåº¦ã€è‡ªåŠ¨æ‰©å®¹ã€æ•…éšœæ¢å¤ã€‚åŠŸèƒ½å¼ºå¤§ï¼Œä½†é…ç½®å¤æ‚ã€‚

ï¼ˆç»§ç»­...ï¼‰
```

**é˜¶æ®µ4ï¼šäººå·¥ç²¾ä¿®**

ä½ éœ€è¦ï¼š
1. è°ƒæ•´è¯­æ°”ï¼ŒåŠ å…¥ä¸ªäººé£æ ¼
2. è¡¥å……å®é™…æ¡ˆä¾‹å’Œæ¼”ç¤ºç¯èŠ‚
3. æ£€æŸ¥æŠ€æœ¯å‡†ç¡®æ€§
4. é¢„æ¼”è„šæœ¬ï¼Œè°ƒæ•´å¡é¡¿çš„åœ°æ–¹

**é˜¶æ®µ5ï¼šå½•åˆ¶è§†é¢‘**

è¿™æ˜¯ç›®å‰æœ€éš¾è‡ªåŠ¨åŒ–çš„ç¯èŠ‚ï¼ˆéœ€è¦ä½ æœ¬äººå‡ºé•œæˆ–å½•å±ï¼‰ã€‚ä½†Agentå¯ä»¥è¾…åŠ©ï¼š

- **è‡ªåŠ¨ç”Ÿæˆå­—å¹•**ï¼šä½¿ç”¨Whisper APIè¯†åˆ«è¯­éŸ³
- **å‰ªè¾‘å»ºè®®**ï¼šåˆ†æè§†é¢‘èŠ‚å¥ï¼Œæ ‡è®°éœ€è¦åŠ å¿«/å‡æ…¢çš„ç‰‡æ®µ
- **B-rollå»ºè®®**ï¼šæ ¹æ®è„šæœ¬ç”Ÿæˆéœ€è¦æ’å…¥çš„èƒŒæ™¯ç”»é¢åˆ—è¡¨

**é˜¶æ®µ6ï¼šä¸Šä¼ å‘å¸ƒ**

```python
async def upload_to_youtube(video_path: str, metadata: dict):
    """
    è‡ªåŠ¨ä¸Šä¼ è§†é¢‘åˆ°YouTube
    """
    
    # 1. ä¸Šä¼ è§†é¢‘æ–‡ä»¶
    video = youtube_api.upload(
        file=video_path,
        title=metadata['title'],
        description=metadata['description'],
        tags=metadata['tags'],
        category="Science & Technology",
        privacy="public"
    )
    
    # 2. ä¸Šä¼ ç¼©ç•¥å›¾
    youtube_api.set_thumbnail(video.id, metadata['thumbnail_path'])
    
    # 3. æ·»åŠ ç« èŠ‚æ ‡è®°ï¼ˆYouTube Chaptersï¼‰
    chapters = parse_chapters(metadata['script'])
    youtube_api.add_chapters(video.id, chapters)
    
    # 4. ç¤¾äº¤åª’ä½“åˆ†å‘
    await twitter.post(f"æ–°è§†é¢‘å‘å¸ƒï¼{metadata['title']} {video.url}")
    await reddit.submit(
        subreddit="learnprogramming",
        title=metadata['title'],
        url=video.url
    )
    
    return video.url
```

**é˜¶æ®µ7ï¼šåˆ†æåé¦ˆ**

```python
async def analyze_video_performance(video_id: str):
    """
    åˆ†æè§†é¢‘è¡¨ç°ï¼Œç”Ÿæˆæ”¹è¿›å»ºè®®
    """
    
    # è·å–æ•°æ®
    stats = await youtube_api.get_video_stats(video_id)
    comments = await youtube_api.get_comments(video_id, max_results=200)
    
    # åˆ†æ
    analysis = await llm.generate(f"""
    è§†é¢‘æ•°æ®ï¼š
    - è§‚çœ‹æ¬¡æ•°ï¼š{stats.views}
    - è§‚çœ‹æ—¶é•¿ï¼š{stats.average_view_duration} / {stats.total_duration}
    - ç‚¹èµç‡ï¼š{stats.likes / stats.views * 100:.2f}%
    - è¯„è®ºæ•°ï¼š{len(comments)}
    
    è¯„è®ºæ‘˜è¦ï¼š
    {summarize_comments(comments)}
    
    è¯·åˆ†æï¼š
    1. è¿™ä¸ªè§†é¢‘è¡¨ç°å¦‚ä½•ï¼Ÿï¼ˆä¸æˆ‘çš„å¹³å‡æ°´å¹³å¯¹æ¯”ï¼‰
    2. è§‚ä¼—æœ€å–œæ¬¢/ä¸å–œæ¬¢å“ªéƒ¨åˆ†ï¼Ÿ
    3. ä¸‹æ¬¡è§†é¢‘åº”è¯¥å¦‚ä½•æ”¹è¿›ï¼Ÿ
    """)
    
    # ä¿å­˜åˆ°çŸ¥è¯†åº“ï¼Œä¾›ä¸‹æ¬¡åˆ›ä½œå‚è€ƒ
    await knowledge_base.add_note(
        title=f"è§†é¢‘åˆ†æï¼š{stats.title}",
        content=analysis,
        tags=["youtube", "analytics"]
    )
    
    return analysis
```

**æ•ˆæœé‡åŒ–**ï¼š

| ç¯èŠ‚ | ä¼ ç»Ÿè€—æ—¶ | AIè¾…åŠ©å | èŠ‚çœ |
|------|---------|---------|------|
| é€‰é¢˜ | 2å°æ—¶ | 15åˆ†é’Ÿ | 87% |
| ç ”ç©¶ | 4å°æ—¶ | 30åˆ†é’Ÿ | 87% |
| è„šæœ¬ | 3å°æ—¶ | 1å°æ—¶ | 67% |
| å½•åˆ¶ | 6å°æ—¶ | 6å°æ—¶ | 0% |
| å‰ªè¾‘ | 4å°æ—¶ | 3å°æ—¶ | 25% |
| å‘å¸ƒ | 1å°æ—¶ | 5åˆ†é’Ÿ | 92% |
| **æ€»è®¡** | **20å°æ—¶** | **~11å°æ—¶** | **45%** |

è™½ç„¶å½•åˆ¶ç¯èŠ‚æ— æ³•è‡ªåŠ¨åŒ–ï¼Œä½†æ•´ä½“æ•ˆç‡æå‡äº†è¿‘ä¸€åŠã€‚æ›´é‡è¦çš„æ˜¯ï¼Œä½ å¯ä»¥å°†èŠ‚çœçš„æ—¶é—´ç”¨äºæå‡è§†é¢‘è´¨é‡ï¼Œè€Œä¸æ˜¯é‡å¤æ€§åŠ³åŠ¨ã€‚

### 9.3.2 æ¡ˆä¾‹2ï¼šå†…å®¹å·¥å‚ï¼ˆDiscordå¤šAgentï¼‰

**åœºæ™¯**ï¼šä½ éœ€è¦æ¯å‘¨äº§å‡º10ç¯‡é«˜è´¨é‡æŠ€æœ¯åšå®¢ï¼Œå•äººæ— æ³•å®Œæˆã€‚

**æ–¹æ¡ˆ**ï¼šåˆ©ç”¨ç¬¬4ç« çš„å¤šAgentæ¶æ„ + ç¬¬5ç« çš„Discordåè°ƒï¼Œæ„å»ºä¸€ä¸ª"è™šæ‹Ÿå†…å®¹å›¢é˜Ÿ"ã€‚

**å›¢é˜Ÿæ¶æ„**ï¼š

```
ä¸»ç¼–Agentï¼ˆä½ ï¼‰ 
â”œâ”€â”€ ç ”ç©¶Agentï¼ˆDiscord #researché¢‘é“ï¼‰
â”œâ”€â”€ å†™ä½œAgentï¼ˆDiscord #draftsé¢‘é“ï¼‰
â”œâ”€â”€ ç¼–è¾‘Agentï¼ˆDiscord #editingé¢‘é“ï¼‰
â””â”€â”€ è®¾è®¡Agentï¼ˆDiscord #graphicsé¢‘é“ï¼‰
```

**å·¥ä½œæµç¨‹**ï¼š

```mermaid
sequenceDiagram
    participant ä¸»ç¼– as ä¸»ç¼–Agent
    participant ç ”ç©¶ as ç ”ç©¶Agent
    participant å†™ä½œ as å†™ä½œAgent
    participant ç¼–è¾‘ as ç¼–è¾‘Agent
    participant è®¾è®¡ as è®¾è®¡Agent
    
    ä¸»ç¼–->>ç ”ç©¶: æ´¾å‘10ä¸ªé€‰é¢˜
    ç ”ç©¶->>ä¸»ç¼–: è¿”å›10å¼ ç ”ç©¶å¡ç‰‡
    ä¸»ç¼–->>å†™ä½œ: æ´¾å‘ç ”ç©¶å¡ç‰‡
    å†™ä½œ->>ä¸»ç¼–: è¿”å›10ç¯‡åˆç¨¿
    ä¸»ç¼–->>ç¼–è¾‘: æ´¾å‘åˆç¨¿
    ç¼–è¾‘->>ä¸»ç¼–: è¿”å›ç²¾ä¿®ç‰ˆ
    ä¸»ç¼–->>è®¾è®¡: æ´¾å‘é…å›¾éœ€æ±‚
    è®¾è®¡->>ä¸»ç¼–: è¿”å›å›¾ç‰‡
    ä¸»ç¼–->>ä¸»ç¼–: æ•´åˆå‘å¸ƒ
```

**å®ç°ç»†èŠ‚**ï¼š

```python
# content_factory.py

class ContentFactory:
    def __init__(self, discord_guild_id: str):
        self.guild_id = discord_guild_id
        self.channels = {
            "research": "1234567890",  # Discordé¢‘é“ID
            "drafts": "1234567891",
            "editing": "1234567892",
            "graphics": "1234567893"
        }
    
    async def produce_articles(self, topics: List[str]) -> List[Article]:
        """
        å¹¶è¡Œç”Ÿäº§å¤šç¯‡æ–‡ç« 
        """
        
        # é˜¶æ®µ1ï¼šå¹¶è¡Œç ”ç©¶
        research_tasks = []
        for topic in topics:
            task_msg = await discord.send_message(
                channel_id=self.channels["research"],
                content=f"@ResearchAgent è¯·ç ”ç©¶ä¸»é¢˜ï¼š{topic}"
            )
            research_tasks.append(task_msg.id)
        
        # ç­‰å¾…æ‰€æœ‰ç ”ç©¶å®Œæˆ
        research_cards = await self.wait_for_responses(
            channel_id=self.channels["research"],
            task_ids=research_tasks,
            timeout=30*60  # 30åˆ†é’Ÿ
        )
        
        # é˜¶æ®µ2ï¼šå¹¶è¡Œå†™ä½œ
        draft_tasks = []
        for i, card in enumerate(research_cards):
            task_msg = await discord.send_message(
                channel_id=self.channels["drafts"],
                content=f"@WritingAgent è¯·åŸºäºä»¥ä¸‹ç ”ç©¶å¡ç‰‡å†™ä½œï¼š\n{card}"
            )
            draft_tasks.append(task_msg.id)
        
        drafts = await self.wait_for_responses(
            channel_id=self.channels["drafts"],
            task_ids=draft_tasks,
            timeout=20*60
        )
        
        # é˜¶æ®µ3ï¼šå¹¶è¡Œç¼–è¾‘
        # ï¼ˆç±»ä¼¼æµç¨‹ï¼‰
        
        # é˜¶æ®µ4ï¼šå¹¶è¡Œé…å›¾
        # ï¼ˆç±»ä¼¼æµç¨‹ï¼‰
        
        return articles
    
    async def wait_for_responses(self, channel_id: str, task_ids: List[str], timeout: int):
        """
        ç­‰å¾…æ‰€æœ‰å­Agentå®Œæˆä»»åŠ¡
        """
        responses = {}
        start_time = time.time()
        
        while len(responses) < len(task_ids):
            if time.time() - start_time > timeout:
                raise TimeoutError(f"éƒ¨åˆ†ä»»åŠ¡è¶…æ—¶ï¼š{set(task_ids) - set(responses.keys())}")
            
            # æ£€æŸ¥é¢‘é“æ–°æ¶ˆæ¯
            messages = await discord.get_messages(channel_id, limit=50)
            
            for msg in messages:
                # æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å¼•ç”¨äº†æŸä¸ªä»»åŠ¡
                if msg.reference and msg.reference.message_id in task_ids:
                    responses[msg.reference.message_id] = msg.content
            
            await asyncio.sleep(10)  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
        
        return list(responses.values())
```

**å­Agenté…ç½®ç¤ºä¾‹**ï¼ˆç ”ç©¶Agentï¼‰ï¼š

```yaml
# research_agent_config.yaml
name: ResearchAgent
model: claude-sonnet-4
persona: |
  ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯ç ”ç©¶å‘˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ·±å…¥ç ”ç©¶ä¸»é¢˜ï¼Œç”Ÿæˆç»“æ„åŒ–ç ”ç©¶å¡ç‰‡ã€‚
  
  å·¥ä½œæµç¨‹ï¼š
  1. ç†è§£ä¸»é¢˜å…³é”®è¯
  2. æŸ¥è¯¢çŸ¥è¯†åº“ + Webæœç´¢
  3. æå–å…³é”®ä¿¡æ¯
  4. ç”Ÿæˆç ”ç©¶å¡ç‰‡ï¼ˆåŒ…å«ï¼šæ ¸å¿ƒæ¦‚å¿µã€æ•°æ®ã€æ¡ˆä¾‹ã€å‚è€ƒæ¥æºï¼‰
  
  è´¨é‡æ ‡å‡†ï¼š
  - äº‹å®å‡†ç¡®ï¼Œæ ‡æ³¨æ¥æº
  - ä¿¡æ¯ä¸°å¯Œï¼Œè‡³å°‘3ä¸ªçœŸå®æ¡ˆä¾‹
  - ç»“æ„æ¸…æ™°ï¼Œæ˜“äºå†™ä½œAgentä½¿ç”¨

triggers:
  - discord_mention: "@ResearchAgent"

output_channel: "#research"
```

**ä¼˜åŠ¿**ï¼š

1. **å¹¶è¡Œå¤„ç†**ï¼š10ç¯‡æ–‡ç« çš„ç ”ç©¶å¯ä»¥åŒæ—¶è¿›è¡Œï¼Œå¤§å¹…ç¼©çŸ­æ€»è€—æ—¶
2. **ä¸“ä¸šåˆ†å·¥**ï¼šæ¯ä¸ªAgentä¸“æ³¨ä¸€ä¸ªç¯èŠ‚ï¼Œæå‡è´¨é‡
3. **é€æ˜å¯è§**ï¼šæ‰€æœ‰å·¥ä½œåœ¨Discordé¢‘é“ä¸­å¯è§ï¼Œæ–¹ä¾¿ç›‘æ§å’Œè°ƒè¯•
4. **æ˜“äºæ‰©å±•**ï¼šéœ€è¦æ›´å¤šäº§å‡ºï¼Ÿå¢åŠ Agentå®ä¾‹å³å¯

**æŒ‘æˆ˜**ï¼š

- **ä¸€è‡´æ€§é—®é¢˜**ï¼šä¸åŒAgentçš„è¾“å‡ºé£æ ¼å¯èƒ½ä¸ä¸€è‡´ï¼ˆéœ€è¦ç»Ÿä¸€é£æ ¼æŒ‡å—ï¼‰
- **é”™è¯¯ä¼ æ’­**ï¼šå¦‚æœç ”ç©¶Agentå‡ºé”™ï¼Œåç»­ç¯èŠ‚éƒ½ä¼šå—å½±å“ï¼ˆéœ€è¦è´¨é‡æ£€æŸ¥ç‚¹ï¼‰
- **æˆæœ¬æ§åˆ¶**ï¼šå¤šä¸ªAgentå¹¶è¡Œå·¥ä½œï¼ŒAPIè°ƒç”¨è´¹ç”¨ä¸Šå‡ï¼ˆéœ€è¦é¢„ç®—ç›‘æ§ï¼‰

ğŸ’¡ **å¤šAgentåä½œçš„å…³é”®åŸåˆ™**
1. **æ˜ç¡®æ¥å£**ï¼šæ¯ä¸ªAgentçš„è¾“å…¥è¾“å‡ºæ ¼å¼è¦æ ‡å‡†åŒ–
2. **å¼‚æ­¥éé˜»å¡**ï¼šä¸è¦è®©ä¸€ä¸ªæ…¢Agentæ‹–ç´¯æ•´ä½“è¿›åº¦
3. **å¤±è´¥å¯æ¢å¤**ï¼šæŸä¸ªä»»åŠ¡å¤±è´¥åï¼Œèƒ½å¤Ÿé‡è¯•æˆ–å›é€€
4. **äººå·¥å¹²é¢„ç‚¹**ï¼šåœ¨å…³é”®ç¯èŠ‚è®¾ç½®äººå·¥å®¡æ ¸ï¼Œé¿å…é”™è¯¯ç´¯ç§¯

### 9.3.3 æ¡ˆä¾‹3ï¼šåšå®¢å‘å¸ƒç®¡é“

**åœºæ™¯**ï¼šä½ ç»´æŠ¤ä¸€ä¸ªæŠ€æœ¯åšå®¢ï¼Œä½¿ç”¨é™æ€ç«™ç‚¹ç”Ÿæˆå™¨ï¼ˆå¦‚Hugo/Jekyllï¼‰ã€‚

**ç›®æ ‡**ï¼šä»å†™ä½œåˆ°å‘å¸ƒå…¨è‡ªåŠ¨åŒ–ï¼ŒåŒ…æ‹¬é…å›¾ã€SEOä¼˜åŒ–ã€ç¤¾äº¤åˆ†å‘ã€‚

**Pipeline**ï¼š

```yaml
# blog_publishing_pipeline.yaml
name: blog-publisher

triggers:
  - file_change: "content/posts/*.md"  # æ£€æµ‹æ–°æ–‡ç« 

steps:
  - name: è´¨é‡æ£€æŸ¥
    action: validate_article
    checks:
      - front_matter_complete  # æ£€æŸ¥å…ƒæ•°æ®å®Œæ•´æ€§
      - word_count: min=1000  # å­—æ•°è¦æ±‚
      - readability_score: min=60  # å¯è¯»æ€§è¯„åˆ†
      - broken_links: false  # æ£€æŸ¥é“¾æ¥æœ‰æ•ˆæ€§
    
  - name: ç”ŸæˆBannerå›¾
    action: generate_image
    params:
      type: banner
      title: "{{ article.title }}"
      style: "{{ config.visual_style }}"
    output: "static/images/{{ article.slug }}.jpg"
  
  - name: SEOä¼˜åŒ–
    action: optimize_seo
    tasks:
      - generate_meta_description
      - suggest_internal_links  # å»ºè®®é“¾æ¥åˆ°ä½ çš„å…¶ä»–æ–‡ç« 
      - optimize_headings  # æ£€æŸ¥H1/H2ç»“æ„
      - generate_alt_text  # ä¸ºå›¾ç‰‡ç”Ÿæˆaltæ–‡æœ¬
  
  - name: æ„å»ºç«™ç‚¹
    action: exec
    command: "hugo --minify"
  
  - name: éƒ¨ç½²åˆ°ç”Ÿäº§
    action: exec
    command: "rsync -avz public/ user@server:/var/www/blog/"
  
  - name: ç¤¾äº¤åª’ä½“åˆ†å‘
    action: parallel
    tasks:
      - twitter_post:
          text: "{{ article.title }} {{ article.url }}"
      - linkedin_post:
          text: "{{ article.excerpt }}"
          url: "{{ article.url }}"
      - reddit_submit:
          subreddit: "{{ article.target_subreddit }}"
          title: "{{ article.title }}"
          url: "{{ article.url }}"
  
  - name: æäº¤æœç´¢å¼•æ“
    action: parallel
    tasks:
      - ping_google_indexing_api
      - ping_bing_indexing_api
  
  - name: é€šçŸ¥å®Œæˆ
    action: telegram_send
    message: |
      âœ… æ–‡ç« å·²å‘å¸ƒæˆåŠŸ
      
      æ ‡é¢˜ï¼š{{ article.title }}
      URLï¼š{{ article.url }}
      å­—æ•°ï¼š{{ article.word_count }}
      
      ç¤¾äº¤åª’ä½“ï¼š
      - Twitter: {{ twitter.url }}
      - LinkedIn: {{ linkedin.url }}
```

**SEOä¼˜åŒ–Agentç¤ºä¾‹**ï¼š

```python
async def optimize_seo(article_md: str, existing_articles: List[str]) -> dict:
    """
    è‡ªåŠ¨ä¼˜åŒ–æ–‡ç« çš„SEO
    """
    
    # 1. ç”Ÿæˆmeta description
    meta_desc = await llm.generate(f"""
    è¯·ä¸ºä»¥ä¸‹æ–‡ç« ç”Ÿæˆä¸€ä¸ª150å­—ä»¥å†…çš„meta descriptionï¼š
    
    {article_md[:500]}
    
    è¦æ±‚ï¼š
    - åŒ…å«å…³é”®è¯
    - å¸å¼•äººç‚¹å‡»
    - å‡†ç¡®æ¦‚æ‹¬å†…å®¹
    """)
    
    # 2. å»ºè®®å†…éƒ¨é“¾æ¥
    # æ‰¾åˆ°æ–‡ç« ä¸­æåˆ°çš„æ¦‚å¿µï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å·²å‘å¸ƒçš„ç›¸å…³æ–‡ç« 
    concepts = extract_concepts(article_md)
    internal_links = []
    
    for concept in concepts:
        matches = search_existing_articles(concept, existing_articles)
        if matches:
            internal_links.append({
                "concept": concept,
                "suggested_link": matches[0].url,
                "anchor_text": matches[0].title
            })
    
    # 3. æ£€æŸ¥H1/H2ç»“æ„
    headings = extract_headings(article_md)
    heading_issues = []
    
    if len([h for h in headings if h.level == 1]) != 1:
        heading_issues.append("åº”è¯¥åªæœ‰ä¸€ä¸ªH1æ ‡é¢˜")
    
    if len([h for h in headings if h.level == 2]) < 3:
        heading_issues.append("å»ºè®®è‡³å°‘3ä¸ªH2å°èŠ‚")
    
    # 4. ä¸ºå›¾ç‰‡ç”Ÿæˆaltæ–‡æœ¬
    images = extract_images(article_md)
    alt_texts = {}
    
    for img in images:
        if not img.alt_text:
            # åŸºäºå›¾ç‰‡ä¸Šä¸‹æ–‡ç”Ÿæˆaltæ–‡æœ¬
            context = get_surrounding_text(article_md, img.position)
            alt = await llm.generate(f"""
            ä¸ºæŠ€æœ¯æ–‡ç« ä¸­çš„å›¾ç‰‡ç”Ÿæˆç®€çŸ­çš„altæ–‡æœ¬ï¼ˆ10-15å­—ï¼‰ï¼š
            
            å›¾ç‰‡å‘¨å›´æ–‡å­—ï¼š
            {context}
            """)
            alt_texts[img.url] = alt
    
    return {
        "meta_description": meta_desc,
        "internal_links": internal_links,
        "heading_issues": heading_issues,
        "alt_texts": alt_texts
    }
```

**ç¤¾äº¤åª’ä½“åˆ†å‘ç­–ç•¥**ï¼š

ä¸åŒå¹³å°éœ€è¦ä¸åŒçš„å†…å®¹æ ¼å¼ï¼š

```python
async def distribute_to_social_media(article: Article):
    """
    é’ˆå¯¹ä¸åŒå¹³å°å®šåˆ¶å†…å®¹
    """
    
    # Twitterï¼šç®€çŸ­ + hook
    twitter_text = await llm.generate(f"""
    å°†æ–‡ç« æ ‡é¢˜è½¬åŒ–ä¸ºå¸å¼•äººçš„Tweetï¼ˆ280å­—ä»¥å†…ï¼‰ï¼š
    
    æ ‡é¢˜ï¼š{article.title}
    æ‘˜è¦ï¼š{article.excerpt}
    
    è¦æ±‚ï¼š
    - æå‡ºé—®é¢˜æˆ–ç—›ç‚¹
    - åŒ…å«1-2ä¸ªemoji
    - ç•™å‡ºç©ºé—´æ”¾é“¾æ¥
    """)
    await twitter.post(f"{twitter_text}\n\n{article.url}")
    
    # LinkedInï¼šä¸“ä¸š + ä»·å€¼
    linkedin_text = await llm.generate(f"""
    å°†æ–‡ç« è½¬åŒ–ä¸ºLinkedInå¸–å­ï¼ˆ500å­—ä»¥å†…ï¼‰ï¼š
    
    {article.markdown}
    
    è¦æ±‚ï¼š
    - ä¸“ä¸šè¯­æ°”
    - å¼ºè°ƒä»·å€¼å’Œæ´å¯Ÿ
    - ç»“å°¾å‘¼åè®¨è®º
    """)
    await linkedin.post(linkedin_text, url=article.url)
    
    # Redditï¼šç¤¾åŒºè´¡çŒ® + è°¦è™š
    reddit_text = await llm.generate(f"""
    å°†æ–‡ç« è½¬åŒ–ä¸ºRedditè‡ªæˆ‘æ¨å¹¿å¸–å­ï¼š
    
    {article.title}
    {article.excerpt}
    
    è¦æ±‚ï¼š
    - è°¦è™šè¯­æ°”ï¼ˆ"æˆ‘å†™äº†..."è€Œä¸æ˜¯"æœ€å…¨æŒ‡å—"ï¼‰
    - è¯´æ˜ä¸ºä»€ä¹ˆè¿™ç¯‡æ–‡ç« å¯¹ç¤¾åŒºæœ‰ä»·å€¼
    - æ¬¢è¿åé¦ˆ
    """)
    await reddit.submit(
        subreddit=article.target_subreddit,
        title=article.title,
        selftext=reddit_text
    )
```

**æ•ˆæœ**ï¼š

- **å‘å¸ƒè€—æ—¶**ï¼šä»60åˆ†é’Ÿé™åˆ°5åˆ†é’Ÿï¼ˆè‡ªåŠ¨åŒ–ååªéœ€æäº¤Markdownæ–‡ä»¶ï¼‰
- **SEOè¡¨ç°**ï¼šè‡ªåŠ¨å†…éƒ¨é“¾æ¥ä½¿é¡µé¢åœç•™æ—¶é—´æå‡30%
- **ç¤¾äº¤æµé‡**ï¼šå®šåˆ¶åŒ–å†…å®¹ä½¿å„å¹³å°ç‚¹å‡»ç‡æå‡50%

ğŸ”§ **å‘å¸ƒPipelineçš„è°ƒè¯•æŠ€å·§**
1. **å…ˆè·‘Dry-runæ¨¡å¼**ï¼šæµ‹è¯•æ‰€æœ‰æ­¥éª¤ä½†ä¸å®é™…å‘å¸ƒ
2. **åˆ†é˜¶æ®µæ¿€æ´»**ï¼šå…ˆè‡ªåŠ¨åŒ–æ„å»ºå’Œéƒ¨ç½²ï¼Œç¨³å®šåå†åŠ å…¥ç¤¾äº¤åˆ†å‘
3. **ä¿ç•™å›æ»šæœºåˆ¶**ï¼šå‡ºé”™æ—¶èƒ½å¿«é€Ÿå›é€€åˆ°ä¸Šä¸€ä¸ªç‰ˆæœ¬
4. **è®°å½•è¯¦ç»†æ—¥å¿—**ï¼šæ¯ä¸ªæ­¥éª¤çš„è¾“å…¥è¾“å‡ºéƒ½è®°å½•ï¼Œæ–¹ä¾¿æ’æŸ¥é—®é¢˜

---

## ğŸ“ˆ 9.4 å¸‚åœºè°ƒç ”ä¸äº§å“å·¥å‚

å†…å®¹åˆ›ä½œçš„ç»ˆæå½¢æ€ä¸æ˜¯"å†™æ–‡ç« "æˆ–"åšè§†é¢‘"ï¼Œè€Œæ˜¯**åˆ›é€ æœ‰ä»·å€¼çš„äº§å“**ã€‚æœ¬èŠ‚ä»‹ç»å¦‚ä½•ç”¨OpenClawæ„å»º"ä»ç—›ç‚¹åˆ°äº§å“"çš„å®Œæ•´å¾ªç¯ã€‚

### 9.4.1 Last 30 Days Skillï¼šæŒ–æ˜çœŸå®ç—›ç‚¹

**ç†å¿µ**ï¼šæœ€å¥½çš„äº§å“ideaä¸æ˜¯é çµå…‰ä¸€ç°ï¼Œè€Œæ˜¯ä»ç”¨æˆ·çš„æ—¥å¸¸æŠ±æ€¨ä¸­æç‚¼å‡ºæ¥çš„ã€‚

**å®ç°**ï¼š

```python
# last_30_days_skill.py

async def discover_pain_points(domain: str, platforms: List[str]) -> List[PainPoint]:
    """
    è¿‡å»30å¤©çš„ç—›ç‚¹æŒ–æ˜
    
    Args:
        domain: ç›®æ ‡é¢†åŸŸï¼ˆå¦‚ "web development", "data science"ï¼‰
        platforms: æ•°æ®æ¥æºï¼ˆå¦‚ ["reddit", "twitter", "hacker_news"]ï¼‰
    """
    
    all_pain_points = []
    
    for platform in platforms:
        if platform == "reddit":
            # æœç´¢RedditæŠ±æ€¨è´´
            posts = await reddit_search(
                query=f'{domain} ("frustrating" OR "difficult" OR "struggling")',
                time_filter="month",
                sort="top",
                limit=100
            )
            
            # æå–ç—›ç‚¹
            for post in posts:
                pain_point = {
                    "text": post.title + "\n" + post.selftext,
                    "upvotes": post.score,
                    "comments": len(post.comments),
                    "url": post.url,
                    "platform": "reddit"
                }
                all_pain_points.append(pain_point)
        
        elif platform == "twitter":
            # æœç´¢TwitteræŠ±æ€¨æ¨æ–‡
            tweets = await twitter_search(
                query=f'{domain} ("why is" OR "how do I" OR "can\'t figure out")',
                result_type="popular",
                count=100
            )
            
            for tweet in tweets:
                pain_point = {
                    "text": tweet.text,
                    "likes": tweet.likes,
                    "retweets": tweet.retweets,
                    "url": tweet.url,
                    "platform": "twitter"
                }
                all_pain_points.append(pain_point)
        
        elif platform == "hacker_news":
            # æœç´¢HNè¯„è®º
            stories = await hn_search(
                query=domain,
                tags="comment",
                num_days=30
            )
            
            # ç­›é€‰"æ±‚åŠ©"ç±»è¯„è®º
            for story in stories:
                if any(keyword in story.text.lower() for keyword in ["help", "issue", "problem"]):
                    pain_point = {
                        "text": story.text,
                        "points": story.points,
                        "url": story.url,
                        "platform": "hacker_news"
                    }
                    all_pain_points.append(pain_point)
    
    # èšç±»ç›¸ä¼¼ç—›ç‚¹
    clustered = await cluster_pain_points(all_pain_points)
    
    # æŒ‰çƒ­åº¦æ’åº
    ranked = rank_pain_points(clustered)
    
    return ranked


async def cluster_pain_points(pain_points: List[dict]) -> List[PainPointCluster]:
    """
    å°†ç›¸ä¼¼çš„ç—›ç‚¹èšç±»
    """
    
    # ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰èšç±»
    clustering_prompt = f"""
    ä»¥ä¸‹æ˜¯ç”¨æˆ·åœ¨è¿‡å»30å¤©æå‡ºçš„ç—›ç‚¹åˆ—è¡¨ã€‚è¯·å°†ç›¸ä¼¼çš„ç—›ç‚¹å½’ç±»ã€‚
    
    {format_pain_points(pain_points)}
    
    è¾“å‡ºæ ¼å¼ï¼š
    ## ç—›ç‚¹ç±»åˆ«1ï¼šXXX
    - åŸå§‹ç—›ç‚¹A
    - åŸå§‹ç—›ç‚¹B
    
    ## ç—›ç‚¹ç±»åˆ«2ï¼šYYY
    - åŸå§‹ç—›ç‚¹C
    - åŸå§‹ç—›ç‚¹D
    """
    
    clusters_text = await llm.generate(clustering_prompt, model="claude-sonnet-4")
    clusters = parse_clusters(clusters_text)
    
    return clusters


def rank_pain_points(clusters: List[PainPointCluster]) -> List[PainPointCluster]:
    """
    æ ¹æ®çƒ­åº¦å’Œå½±å“åŠ›å¯¹ç—›ç‚¹æ’åº
    """
    
    for cluster in clusters:
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        cluster.score = (
            sum(p["upvotes"] for p in cluster.pain_points) * 1.0 +  # Reddit/HN upvotes
            sum(p["likes"] for p in cluster.pain_points) * 0.5 +     # Twitter likes
            len(cluster.pain_points) * 10                            # æåŠæ¬¡æ•°
        )
    
    return sorted(clusters, key=lambda c: c.score, reverse=True)
```

**è¾“å‡ºç¤ºä¾‹**ï¼š

```markdown
# è¿‡å»30å¤©ç—›ç‚¹æŒ–æ˜ï¼šWeb Development

## ç—›ç‚¹ç±»åˆ«1ï¼šReactçŠ¶æ€ç®¡ç†å¤æ‚åº¦ï¼ˆè¯„åˆ†ï¼š2,340ï¼‰
æåŠæ¬¡æ•°ï¼š47æ¬¡ | å¹³å°ï¼šReddit(32) Twitter(15)

**å…¸å‹æè¿°**ï¼š
- "ä¸ºä»€ä¹ˆReactçŠ¶æ€ç®¡ç†è¿™ä¹ˆå¤æ‚ï¼ŸReduxã€MobXã€Zustandã€Recoil...åˆ°åº•è¯¥ç”¨å“ªä¸ªï¼Ÿ"ï¼ˆ234 upvotesï¼‰
- "æˆ‘çš„ç»„ä»¶åµŒå¥—5å±‚ï¼ŒçŠ¶æ€ä¼ é€’å·²ç»å¤±æ§äº†"ï¼ˆ189 upvotesï¼‰
- "æ¯æ¬¡å­¦ä¸€ä¸ªæ–°çš„çŠ¶æ€ç®¡ç†åº“ï¼Œè¿‡åŠå¹´å°±è¿‡æ—¶äº†"ï¼ˆ156 upvotesï¼‰

**ç”¨æˆ·ç—›ç‚¹åˆ†æ**ï¼š
- é€‰æ‹©å›°éš¾ï¼šå·¥å…·å¤ªå¤šï¼Œä¸çŸ¥é“é€‰å“ªä¸ª
- å­¦ä¹ æˆæœ¬ï¼šæ¯ä¸ªåº“çš„å¿ƒæ™ºæ¨¡å‹ä¸åŒ
- ç»´æŠ¤è´Ÿæ‹…ï¼šé¡¹ç›®å˜å¤§åçŠ¶æ€éš¾ç®¡ç†

**æ½œåœ¨è§£å†³æ–¹æ¡ˆ**ï¼š
- äº¤äº’å¼å†³ç­–æ ‘å·¥å…·ï¼š"å›ç­”3ä¸ªé—®é¢˜ï¼Œæ¨èæœ€é€‚åˆä½ çš„çŠ¶æ€ç®¡ç†æ–¹æ¡ˆ"
- çŠ¶æ€ç®¡ç†å¯è§†åŒ–è°ƒè¯•å™¨
- ç»Ÿä¸€çš„çŠ¶æ€ç®¡ç†æŠ½è±¡å±‚

---

## ç—›ç‚¹ç±»åˆ«2ï¼šCSSå“åº”å¼å¸ƒå±€è°ƒè¯•ï¼ˆè¯„åˆ†ï¼š1,890ï¼‰
æåŠæ¬¡æ•°ï¼š38æ¬¡ | å¹³å°ï¼šReddit(25) Twitter(13)

**å…¸å‹æè¿°**ï¼š
- "ä¸ºä»€ä¹ˆæˆ‘çš„CSSåœ¨iPhoneä¸Šæ˜¾ç¤ºæ­£å¸¸ï¼Œåœ¨iPadä¸Šå°±ä¹±äº†ï¼Ÿ"ï¼ˆ201 upvotesï¼‰
- "è°ƒè¯•å“åº”å¼å¸ƒå±€å¤ªç—›è‹¦ï¼Œæ”¹äº†è¿™ä¸ªæ–­ç‚¹ï¼Œé‚£ä¸ªæ–­ç‚¹åˆåäº†"ï¼ˆ178 upvotesï¼‰

ï¼ˆç»§ç»­...ï¼‰
```

ğŸ“š **çœŸå®æ¡ˆä¾‹ï¼šLinearçš„è¯ç”Ÿ**
Linearï¼ˆé¡¹ç›®ç®¡ç†å·¥å…·ï¼‰çš„åˆ›å§‹äººKarri Saarinenåœ¨é‡‡è®¿ä¸­æåˆ°ï¼šä»–ä»¬å›¢é˜Ÿåœ¨ä½¿ç”¨Jiraæ—¶ï¼Œå‘ç°æœ€å¤§çš„ç—›ç‚¹æ˜¯"é€Ÿåº¦æ…¢"å’Œ"ç•Œé¢è‡ƒè‚¿"ã€‚é€šè¿‡åˆ†æHacker Newså’ŒRedditä¸Šæ•°ç™¾æ¡å…³äºJiraçš„æŠ±æ€¨ï¼Œä»–ä»¬ç¡®è®¤è¿™æ˜¯æ™®ééœ€æ±‚ï¼Œè€Œä¸æ˜¯ä¸ªåˆ«ç°è±¡ã€‚

è¿™éªŒè¯äº†äº§å“æ–¹å‘ï¼Œè®©ä»–ä»¬æœ‰ä¿¡å¿ƒæŠ•å…¥ä¸€å¹´æ—¶é—´å¼€å‘Linearã€‚ä»Šå¤©Linearä¼°å€¼è¶…è¿‡10äº¿ç¾å…ƒã€‚

### 9.4.2 ä»ç—›ç‚¹åˆ°MVPï¼šè‡ªåŠ¨åŒ–äº§å“éªŒè¯

å‘ç°ç—›ç‚¹åï¼Œä¼ ç»Ÿåšæ³•æ˜¯ï¼šå†™PRD â†’ æ‰¾å¼€å‘ â†’ èŠ±æ•°æœˆå¼€å‘ â†’ ä¸Šçº¿åå‘ç°éœ€æ±‚ç†è§£é”™äº†ã€‚

OpenClawçš„æ–¹æ¡ˆï¼š**è®©AI Agentåœ¨ä¸€å¤œä¹‹é—´æ„å»ºMVPï¼Œå¿«é€ŸéªŒè¯å‡è®¾ã€‚**

**Goal-driven Autonomous Tasks**ï¼š

```python
# overnight_app_builder.py

async def build_mvp_from_pain_point(pain_point: PainPointCluster):
    """
    ä»ç—›ç‚¹è‡ªåŠ¨æ„å»ºMVP
    """
    
    # æ­¥éª¤1ï¼šç”Ÿæˆäº§å“æ–¹æ¡ˆ
    product_spec = await llm.generate(f"""
    ç”¨æˆ·ç—›ç‚¹ï¼š
    {pain_point.description}
    
    è¯·è®¾è®¡ä¸€ä¸ªæœ€å°åŒ–å¯è¡Œäº§å“ï¼ˆMVPï¼‰ï¼Œè¦æ±‚ï¼š
    1. æ ¸å¿ƒåŠŸèƒ½åªè§£å†³æœ€ç—›çš„é‚£ä¸ªç‚¹
    2. å¯ä»¥åœ¨24å°æ—¶å†…å®ç°
    3. ä½¿ç”¨ç°æœ‰å·¥å…·/APIï¼Œä¸ä»é›¶å¼€å‘
    
    è¾“å‡ºæ ¼å¼ï¼š
    ## äº§å“å®šä½
    ä¸€å¥è¯æè¿°
    
    ## æ ¸å¿ƒåŠŸèƒ½
    åˆ—ä¸¾3-5ä¸ªåŠŸèƒ½ç‚¹
    
    ## æŠ€æœ¯æ ˆ
    å‰ç«¯ã€åç«¯ã€æ•°æ®åº“ã€ç¬¬ä¸‰æ–¹API
    
    ## å®ç°è·¯å¾„
    åˆ†æ­¥éª¤è¯´æ˜å¦‚ä½•å®ç°
    """)
    
    # æ­¥éª¤2ï¼šåˆ†è§£ä»»åŠ¡
    tasks = await decompose_tasks(product_spec)
    
    # æ­¥éª¤3ï¼šè‡ªä¸»æ‰§è¡Œ
    for task in tasks:
        await execute_task_autonomously(task)
    
    # æ­¥éª¤4ï¼šéƒ¨ç½²ä¸Šçº¿
    await deploy_mvp()
    
    # æ­¥éª¤5ï¼šç”Ÿæˆè½åœ°é¡µ
    landing_page = await generate_landing_page(product_spec)
    
    return {
        "product_url": "https://mvp.example.com",
        "landing_page_url": "https://mvp.example.com/landing",
        "source_code": "https://github.com/user/mvp-repo"
    }


async def decompose_tasks(product_spec: str) -> List[Task]:
    """
    å°†äº§å“æ–¹æ¡ˆåˆ†è§£ä¸ºå¯æ‰§è¡Œä»»åŠ¡
    """
    
    decomposition = await llm.generate(f"""
    äº§å“æ–¹æ¡ˆï¼š
    {product_spec}
    
    è¯·åˆ†è§£ä¸ºå…·ä½“çš„å¼€å‘ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡åº”è¯¥ï¼š
    - å¯ä»¥ç‹¬ç«‹å®Œæˆ
    - æœ‰æ˜ç¡®çš„è¾“å…¥å’Œè¾“å‡º
    - è€—æ—¶ä¸è¶…è¿‡2å°æ—¶
    
    è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
    [
      {{
        "name": "åˆ›å»ºæ•°æ®åº“Schema",
        "type": "code",
        "dependencies": [],
        "estimated_time": "30min"
      }},
      {{
        "name": "å®ç°APIç«¯ç‚¹",
        "type": "code",
        "dependencies": ["åˆ›å»ºæ•°æ®åº“Schema"],
        "estimated_time": "1h"
      }}
    ]
    """)
    
    tasks = json.loads(decomposition)
    return tasks


async def execute_task_autonomously(task: Task):
    """
    è‡ªä¸»æ‰§è¡Œå•ä¸ªä»»åŠ¡
    """
    
    if task.type == "code":
        # ç”Ÿæˆä»£ç 
        code = await llm.generate(f"""
        ä»»åŠ¡ï¼š{task.name}
        
        è¯·ç”Ÿæˆå®Œæ•´çš„ã€å¯è¿è¡Œçš„ä»£ç ã€‚
        
        è¦æ±‚ï¼š
        - åŒ…å«å¿…è¦çš„é”™è¯¯å¤„ç†
        - æ·»åŠ æ³¨é‡Šè¯´æ˜å…³é”®é€»è¾‘
        - ç¬¦åˆæœ€ä½³å®è·µ
        """, model="claude-sonnet-4")
        
        # ä¿å­˜ä»£ç 
        await write_file(task.output_path, code)
        
        # è¿è¡Œæµ‹è¯•
        test_result = await run_tests(task.output_path)
        
        if not test_result.passed:
            # è‡ªåŠ¨ä¿®å¤
            fixed_code = await llm.generate(f"""
            ä»£ç ï¼š
            {code}
            
            æµ‹è¯•å¤±è´¥ï¼š
            {test_result.errors}
            
            è¯·ä¿®å¤é”™è¯¯ã€‚
            """)
            await write_file(task.output_path, fixed_code)
    
    elif task.type == "design":
        # ç”ŸæˆUIè®¾è®¡
        design = await generate_ui_design(task.description)
        await save_design(task.output_path, design)
    
    elif task.type == "content":
        # ç”Ÿæˆæ–‡æ¡ˆ
        content = await llm.generate(task.prompt)
        await save_content(task.output_path, content)
```

**å®æˆ˜æ¡ˆä¾‹ï¼šReactçŠ¶æ€ç®¡ç†å†³ç­–æ ‘**

åŸºäºå‰é¢æŒ–æ˜çš„ç—›ç‚¹ï¼Œæˆ‘ä»¬è®©Agentè‡ªåŠ¨æ„å»ºä¸€ä¸ªMVPï¼š

```markdown
## äº§å“å®šä½
ReactçŠ¶æ€ç®¡ç†é€‰æ‹©åŠ©æ‰‹ï¼šå›ç­”3ä¸ªé—®é¢˜ï¼ŒAIæ¨èæœ€é€‚åˆä½ çš„æ–¹æ¡ˆ

## æ ¸å¿ƒåŠŸèƒ½
1. äº¤äº’å¼é—®å·ï¼ˆ3-5ä¸ªé—®é¢˜ï¼‰
2. åŸºäºå›ç­”æ¨èæ–¹æ¡ˆï¼ˆRedux/Zustand/Recoil/Context APIï¼‰
3. æ˜¾ç¤ºæ¨èç†ç”±å’Œä»£ç ç¤ºä¾‹
4. æä¾›å­¦ä¹ èµ„æºé“¾æ¥

## æŠ€æœ¯æ ˆ
- å‰ç«¯ï¼šNext.js + TailwindCSS
- åç«¯ï¼šVercel Serverless Functions
- AIæ¨ç†ï¼šOpenAI GPT-4
- éƒ¨ç½²ï¼šVercel

## å®ç°è·¯å¾„
1. åˆ›å»ºNext.jsé¡¹ç›®è„šæ‰‹æ¶
2. è®¾è®¡é—®å·æµç¨‹ï¼ˆJSONé…ç½®ï¼‰
3. å®ç°æ¨èç®—æ³•ï¼ˆè°ƒç”¨GPT-4ï¼‰
4. åˆ›å»ºç»“æœå±•ç¤ºé¡µé¢
5. éƒ¨ç½²åˆ°Vercel
```

Agentåœ¨6å°æ—¶å†…å®Œæˆäº†æ‰€æœ‰ä»£ç ï¼Œéƒ¨ç½²ä¸Šçº¿åï¼š

- **ç¬¬1å¤©**ï¼šåˆ†äº«åˆ°Reddit r/reactjsï¼Œè·å¾—120 upvotes
- **ç¬¬1å‘¨**ï¼šç´¯è®¡2,300æ¬¡è®¿é—®
- **ç¬¬2å‘¨**ï¼šæœ‰äººåœ¨GitHubæPRæ”¹è¿›é—®å·é€»è¾‘

è¿™éªŒè¯äº†éœ€æ±‚çœŸå®å­˜åœ¨ã€‚å¦‚æœåå“å¹³å¹³ï¼Œå¯ä»¥å¿«é€Ÿæ”¾å¼ƒï¼Œæˆæœ¬åªæœ‰å‡ å°æ—¶ã€‚

ğŸ’¡ **MVPéªŒè¯çš„é»„é‡‘æŒ‡æ ‡**
ä¸è¦çœ‹ç»å¯¹æ•°å­—ï¼ˆ"æœ‰å¤šå°‘ç”¨æˆ·"ï¼‰ï¼Œè€Œè¦çœ‹ç›¸å¯¹æŒ‡æ ‡ï¼š
- **è½¬åŒ–ç‡**ï¼šè®¿é—®è€…ä¸­æœ‰å¤šå°‘äººå®Œæˆæ ¸å¿ƒåŠ¨ä½œï¼Ÿï¼ˆ>20%è¯´æ˜éœ€æ±‚å¼ºï¼‰
- **è‡ªå‘åˆ†äº«**ï¼šæœ‰å¤šå°‘äººä¸»åŠ¨åˆ†äº«ç»™æœ‹å‹ï¼Ÿï¼ˆ>5%è¯´æ˜äº§å“æœ‰ä¼ æ’­åŠ›ï¼‰
- **æ·±åº¦å‚ä¸**ï¼šæœ‰å¤šå°‘äººåå¤ä½¿ç”¨æˆ–æå‡ºæ”¹è¿›å»ºè®®ï¼Ÿï¼ˆ>2%è¯´æ˜æœ‰ç²˜æ€§ï¼‰

### 9.4.3 éªŒè¯å¾ªç¯ï¼šå¿«é€Ÿè¿­ä»£

MVPä¸Šçº¿åï¼Œè¿›å…¥éªŒè¯å¾ªç¯ï¼š

```mermaid
graph LR
    A[å‘å¸ƒMVP] --> B[æ”¶é›†åé¦ˆ]
    B --> C[åˆ†ææ•°æ®]
    C --> D{éœ€æ±‚éªŒè¯?}
    D -->|æ˜¯| E[æŠ•å…¥æ·±åº¦å¼€å‘]
    D -->|å¦| F[è°ƒæ•´æ–¹å‘]
    F --> A
    D -->|ä¸ç¡®å®š| G[ä¼˜åŒ–MVP]
    G --> A
```

**è‡ªåŠ¨åŒ–åé¦ˆæ”¶é›†**ï¼š

```python
async def collect_user_feedback(product_url: str):
    """
    ä»å¤šä¸ªæ¸ é“æ”¶é›†ç”¨æˆ·åé¦ˆ
    """
    
    feedback = {
        "analytics": await get_analytics_data(product_url),
        "social_mentions": await search_social_mentions(product_url),
        "direct_feedback": await get_feedback_form_submissions(product_url)
    }
    
    # åˆ†æåé¦ˆ
    analysis = await llm.generate(f"""
    äº§å“åé¦ˆæ•°æ®ï¼š
    
    Analyticsï¼š
    - è®¿é—®é‡ï¼š{feedback['analytics']['pageviews']}
    - è½¬åŒ–ç‡ï¼š{feedback['analytics']['conversion_rate']}%
    - å¹³å‡åœç•™æ—¶é—´ï¼š{feedback['analytics']['avg_time_on_page']}ç§’
    
    ç¤¾äº¤åª’ä½“æåŠï¼š
    {format_social_mentions(feedback['social_mentions'])}
    
    ç”¨æˆ·ç›´æ¥åé¦ˆï¼š
    {format_feedback_submissions(feedback['direct_feedback'])}
    
    è¯·åˆ†æï¼š
    1. ç”¨æˆ·æœ€å–œæ¬¢çš„åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ
    2. æœ€å¤§çš„ä¸æ»¡æ˜¯ä»€ä¹ˆï¼Ÿ
    3. å»ºè®®ä¸‹ä¸€æ­¥ä¼˜åŒ–æ–¹å‘
    4. åˆ¤æ–­ï¼šéœ€æ±‚æ˜¯å¦å¾—åˆ°éªŒè¯ï¼Ÿï¼ˆå¼ºéªŒè¯/å¼±éªŒè¯/æœªéªŒè¯ï¼‰
    """)
    
    return analysis
```

**å¿«é€Ÿè¿­ä»£ç¤ºä¾‹**ï¼š

| è¿­ä»£ | å‡è®¾ | MVP | ç»“æœ | è¡ŒåŠ¨ |
|------|------|-----|------|------|
| 1 | ç”¨æˆ·éœ€è¦çŠ¶æ€ç®¡ç†å†³ç­–æ ‘ | 3ä¸ªé—®é¢˜ + æ¨è | è½¬åŒ–ç‡25% | éªŒè¯é€šè¿‡ï¼Œç»§ç»­ |
| 2 | ç”¨æˆ·æƒ³çœ‹ä»£ç ç¤ºä¾‹ | å¢åŠ ä»£ç æ¨¡æ¿ | åœç•™æ—¶é—´+50% | éªŒè¯é€šè¿‡ï¼Œç»§ç»­ |
| 3 | ç”¨æˆ·æƒ³å¯¹æ¯”å¤šä¸ªæ–¹æ¡ˆ | å¢åŠ å¯¹æ¯”è¡¨æ ¼ | è·³å‡ºç‡é™ä½30% | éªŒè¯é€šè¿‡ï¼Œç»§ç»­ |
| 4 | ç”¨æˆ·éœ€è¦è§†é¢‘æ•™ç¨‹ | åµŒå…¥YouTubeè§†é¢‘ | æ— æ˜æ˜¾å˜åŒ– | å‡è®¾é”™è¯¯ï¼Œå›é€€ |

é€šè¿‡å¿«é€Ÿè¿­ä»£ï¼Œä½ å¯ä»¥åœ¨å‡ å‘¨å†…æ‰¾åˆ°Product-Market Fitï¼Œè€Œä¸æ˜¯èŠ±å‡ ä¸ªæœˆå¼€å‘ä¸€ä¸ªæ²¡äººè¦çš„äº§å“ã€‚

ğŸ”§ **é¿å…è¿‡åº¦ä¼˜åŒ–é™·é˜±**
MVPçš„ç›®æ ‡æ˜¯**éªŒè¯å‡è®¾**ï¼Œè€Œä¸æ˜¯"åšä¸€ä¸ªå®Œç¾çš„äº§å“"ã€‚å¸¸è§é™·é˜±ï¼š
- çº ç»“UIç»†èŠ‚ï¼ˆç°é˜¶æ®µç”¨æˆ·ä¸åœ¨ä¹ï¼‰
- æ·»åŠ "ä¹Ÿè®¸æœ‰ç”¨"çš„åŠŸèƒ½ï¼ˆåˆ†æ•£ç„¦ç‚¹ï¼‰
- è¿‡åº¦ä¼˜åŒ–æ€§èƒ½ï¼ˆç”¨æˆ·é‡è¿˜ä¸å¤Ÿå¤§ï¼‰

è®°ä½ï¼šMVPçš„ä»·å€¼åœ¨äº**å¿«é€Ÿå¤±è´¥**æˆ–**å¿«é€ŸéªŒè¯**ï¼Œè€Œä¸æ˜¯"åšä¸€ä¸ªåŠŸèƒ½é½å…¨çš„äº§å“"ã€‚

---

## ğŸ¨ 9.5 è®¾è®¡ä½ çš„å†…å®¹ç³»ç»Ÿ

å‰é¢æˆ‘ä»¬è®²äº†é€šç”¨çš„Pipelineå’Œæ¡ˆä¾‹ï¼Œç°åœ¨æ¥è°ˆè°ˆå¦‚ä½•æ ¹æ®ä½ çš„å…·ä½“æƒ…å†µè®¾è®¡å†…å®¹ç³»ç»Ÿã€‚

### 9.5.1 æ ¹æ®å†…å®¹ç±»å‹è°ƒæ•´Pipeline

ä¸åŒç±»å‹çš„å†…å®¹ï¼ŒPipelineä¾§é‡ç‚¹ä¸åŒï¼š

**æ•™ç¨‹ç±»å†…å®¹**ï¼ˆå¦‚æŠ€æœ¯åšå®¢ã€è§†é¢‘æ•™ç¨‹ï¼‰ï¼š

```
é€‰é¢˜ï¼ˆ20%é‡è¦æ€§ï¼‰â†’ ç ”ç©¶ï¼ˆ40%ï¼‰â†’ èµ·è‰ï¼ˆ30%ï¼‰â†’ ç¼–è¾‘ï¼ˆ10%ï¼‰
```

- **ç ”ç©¶æœ€å…³é”®**ï¼šéœ€è¦ç¡®ä¿æŠ€æœ¯å‡†ç¡®æ€§ï¼Œæä¾›å¯è¿è¡Œçš„ä»£ç 
- **èµ·è‰å¯ä»¥AIè¾…åŠ©**ï¼šå› ä¸ºç»“æ„ç›¸å¯¹å›ºå®šï¼ˆé—®é¢˜â†’æ–¹æ¡ˆâ†’ä»£ç â†’æ€»ç»“ï¼‰
- **ç¼–è¾‘ä¸»è¦æ£€æŸ¥äº‹å®**ï¼šè€Œä¸æ˜¯æ¶¦è‰²æ–‡ç¬”

**è§‚ç‚¹ç±»å†…å®¹**ï¼ˆå¦‚è¯„è®ºæ–‡ç« ã€è¡Œä¸šåˆ†æï¼‰ï¼š

```
é€‰é¢˜ï¼ˆ40%ï¼‰â†’ ç ”ç©¶ï¼ˆ30%ï¼‰â†’ èµ·è‰ï¼ˆ10%ï¼‰â†’ ç¼–è¾‘ï¼ˆ20%ï¼‰
```

- **é€‰é¢˜æœ€å…³é”®**ï¼šéœ€è¦æ‰¾åˆ°ç‹¬ç‰¹è§’åº¦ï¼ŒAIåªèƒ½è¾…åŠ©è€Œä¸èƒ½æ›¿ä»£
- **ç ”ç©¶æä¾›è®ºæ®**ï¼šæ”¶é›†æ•°æ®å’Œæ¡ˆä¾‹æ”¯æŒè§‚ç‚¹
- **èµ·è‰AIä½œç”¨æœ‰é™**ï¼šæ ¸å¿ƒè§‚ç‚¹å¿…é¡»æ¥è‡ªä½ 
- **ç¼–è¾‘æ‰“ç£¨è®ºè¿°**ï¼šè®©è®ºè¯æ›´æœ‰è¯´æœåŠ›

**å¨±ä¹ç±»å†…å®¹**ï¼ˆå¦‚æç¬‘è§†é¢‘ã€memeï¼‰ï¼š

```
é€‰é¢˜ï¼ˆ50%ï¼‰â†’ ç ”ç©¶ï¼ˆ10%ï¼‰â†’ èµ·è‰ï¼ˆ20%ï¼‰â†’ ç¼–è¾‘ï¼ˆ20%ï¼‰
```

- **é€‰é¢˜å°±æ˜¯ä¸€åˆ‡**ï¼šåˆ›æ„å†³å®šæˆè´¥
- **ç ”ç©¶å¯ä»¥è·³è¿‡**ï¼šä¸éœ€è¦ä¸¥è°¨çš„äº‹å®æ ¸æŸ¥
- **èµ·è‰å’Œç¼–è¾‘éƒ½æ˜¯æ‰§è¡Œ**ï¼šç›¸å¯¹æœºæ¢°

æ ¹æ®ä½ çš„å†…å®¹ç±»å‹ï¼Œè°ƒæ•´å„é˜¶æ®µçš„æ—¶é—´åˆ†é…å’Œè‡ªåŠ¨åŒ–ç¨‹åº¦ã€‚

### 9.5.2 äººå·¥ä»‹å…¥ç‚¹çš„è®¾è®¡

è‡ªåŠ¨åŒ–ä¸ç­‰äº"è®©AIåšæ‰€æœ‰äº‹"ã€‚å…³é”®æ˜¯è®¾è®¡å¥½**äººå·¥ä»‹å…¥ç‚¹**ï¼ˆHuman-in-the-Loopï¼‰ã€‚

**ä¸‰ç§ä»‹å…¥ç­–ç•¥**ï¼š

**1. å‰ç½®å®¡æ ¸ï¼ˆPre-approvalï¼‰**

```python
async def content_pipeline_with_preapproval(topic: str):
    # AIç”Ÿæˆé€‰é¢˜å»ºè®®
    topic_ideas = await generate_topic_ideas(topic)
    
    # ğŸš¦ äººå·¥é€‰æ‹©
    selected_topic = await human_select(topic_ideas)
    
    # AIç ”ç©¶
    research = await research_topic(selected_topic)
    
    # ğŸš¦ äººå·¥å®¡æ ¸ç ”ç©¶å¡ç‰‡
    approved_research = await human_review(research)
    
    # AIç”Ÿæˆåˆç¨¿
    draft = await generate_draft(approved_research)
    
    # è‡ªåŠ¨å‘å¸ƒï¼ˆå·²ç»å®¡æ ¸è¿‡å…³é”®ç¯èŠ‚ï¼‰
    await publish(draft)
```

**é€‚ç”¨åœºæ™¯**ï¼šé«˜é£é™©å†…å®¹ï¼ˆå¦‚å…¬å¸å®˜æ–¹åšå®¢ã€ä»˜è´¹è¯¾ç¨‹ï¼‰

**2. åç½®å®¡æ ¸ï¼ˆPost-approvalï¼‰**

```python
async def content_pipeline_with_postapproval(topic: str):
    # AIå…¨è‡ªåŠ¨ç”Ÿæˆ
    draft = await full_auto_generate(topic)
    
    # ğŸš¦ äººå·¥æœ€ç»ˆå®¡æ ¸
    approved_draft = await human_final_review(draft)
    
    # å‘å¸ƒ
    await publish(approved_draft)
```

**é€‚ç”¨åœºæ™¯**ï¼šä¸­ç­‰é£é™©å†…å®¹ï¼ˆå¦‚ä¸ªäººåšå®¢ã€ç¤¾äº¤åª’ä½“å¸–å­ï¼‰

**3. æŠ½æ ·å®¡æ ¸ï¼ˆSamplingï¼‰**

```python
async def content_pipeline_with_sampling(topics: List[str]):
    # AIæ‰¹é‡ç”Ÿæˆ
    drafts = await parallel_generate(topics)
    
    # ğŸš¦ äººå·¥æŠ½æŸ¥10%
    sample = random.sample(drafts, k=len(drafts)//10)
    issues = await human_spot_check(sample)
    
    if issues.count > threshold:
        # å‘ç°é—®é¢˜è¾ƒå¤šï¼Œå…¨éƒ¨äººå·¥å®¡æ ¸
        await human_review_all(drafts)
    else:
        # è´¨é‡ç¨³å®šï¼Œç›´æ¥å‘å¸ƒ
        await publish_all(drafts)
```

**é€‚ç”¨åœºæ™¯**ï¼šä½é£é™©ã€å¤§æ‰¹é‡å†…å®¹ï¼ˆå¦‚ç¤¾äº¤åª’ä½“è‡ªåŠ¨å›å¤ã€Newsletteræ‘˜è¦ï¼‰

### 9.5.3 è´¨é‡æ§åˆ¶æ£€æŸ¥æ¸…å•

ä¸ºäº†ç¡®ä¿AIç”Ÿæˆå†…å®¹çš„è´¨é‡ï¼Œå»ºç«‹ä¸€å¥—è‡ªåŠ¨åŒ–æ£€æŸ¥æ¸…å•ï¼š

```python
async def quality_check(article: Article) -> QualityReport:
    """
    å†…å®¹è´¨é‡è‡ªåŠ¨æ£€æŸ¥
    """
    
    issues = []
    
    # 1. äº‹å®å‡†ç¡®æ€§æ£€æŸ¥
    facts = extract_factual_claims(article.content)
    for fact in facts:
        verification = await verify_fact(fact)
        if not verification.confident:
            issues.append(f"âš ï¸ äº‹å®éœ€è¦æ ¸å®ï¼š{fact}")
    
    # 2. é“¾æ¥æœ‰æ•ˆæ€§
    links = extract_links(article.content)
    for link in links:
        if not await is_link_valid(link):
            issues.append(f"ğŸ”— é“¾æ¥å¤±æ•ˆï¼š{link}")
    
    # 3. å¯è¯»æ€§è¯„åˆ†
    readability = calculate_readability_score(article.content)
    if readability < 60:
        issues.append(f"ğŸ“– å¯è¯»æ€§åä½ï¼š{readability}/100")
    
    # 4. åŸåˆ›æ€§æ£€æŸ¥
    similarity = await check_plagiarism(article.content)
    if similarity > 0.3:
        issues.append(f"âš ï¸ ä¸å·²æœ‰å†…å®¹ç›¸ä¼¼åº¦è¿‡é«˜ï¼š{similarity*100:.1f}%")
    
    # 5. SEOåŸºç¡€æ£€æŸ¥
    if not article.meta_description:
        issues.append("ğŸ“Š ç¼ºå°‘meta description")
    if len(article.title) > 60:
        issues.append(f"ğŸ“Š æ ‡é¢˜è¿‡é•¿ï¼ˆ{len(article.title)}å­—ç¬¦ï¼‰")
    
    # 6. ä»£ç å¯è¿è¡Œæ€§ï¼ˆå¦‚æœåŒ…å«ä»£ç ï¼‰
    code_blocks = extract_code_blocks(article.content)
    for code in code_blocks:
        if code.language in ["python", "javascript", "bash"]:
            execution_result = await try_run_code(code)
            if execution_result.error:
                issues.append(f"ğŸ’» ä»£ç æ— æ³•è¿è¡Œï¼š{code.preview}")
    
    return QualityReport(
        passed=len(issues) == 0,
        issues=issues,
        score=calculate_quality_score(issues)
    )
```

### 9.5.4 æŒç»­ä¼˜åŒ–ï¼šä»æ•°æ®ä¸­å­¦ä¹ 

æœ€åï¼Œä½ çš„å†…å®¹ç³»ç»Ÿåº”è¯¥èƒ½å¤Ÿ**è‡ªæˆ‘è¿›åŒ–**ï¼š

```python
async def learn_from_performance():
    """
    åˆ†æå·²å‘å¸ƒå†…å®¹çš„è¡¨ç°ï¼Œä¼˜åŒ–æœªæ¥ç”Ÿäº§
    """
    
    # 1. è·å–æ‰€æœ‰å·²å‘å¸ƒå†…å®¹çš„æ•°æ®
    articles = await get_published_articles(days=90)
    
    for article in articles:
        article.performance = await get_analytics(article.url)
    
    # 2. æ‰¾å‡ºé«˜è¡¨ç°å’Œä½è¡¨ç°çš„å†…å®¹
    top_performers = sorted(articles, key=lambda a: a.performance.score)[-10:]
    low_performers = sorted(articles, key=lambda a: a.performance.score)[:10:]
    
    # 3. åˆ†æå·®å¼‚
    analysis = await llm.generate(f"""
    é«˜è¡¨ç°å†…å®¹ç‰¹å¾ï¼š
    {summarize_articles(top_performers)}
    
    ä½è¡¨ç°å†…å®¹ç‰¹å¾ï¼š
    {summarize_articles(low_performers)}
    
    è¯·åˆ†æï¼š
    1. é«˜è¡¨ç°å†…å®¹æœ‰å“ªäº›å…±åŒç‰¹ç‚¹ï¼Ÿï¼ˆé€‰é¢˜ã€ç»“æ„ã€é•¿åº¦ã€é£æ ¼ï¼‰
    2. ä½è¡¨ç°å†…å®¹çš„ä¸»è¦é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
    3. æä¾›3-5æ¡å…·ä½“çš„æ”¹è¿›å»ºè®®
    """)
    
    # 4. æ›´æ–°å†…å®¹ç”Ÿäº§è§„åˆ™
    await update_content_guidelines(analysis)
    
    # 5. A/Bæµ‹è¯•æ–°è§„åˆ™
    await schedule_ab_test(
        variant_a="current_guidelines",
        variant_b="new_guidelines",
        duration_days=14
    )
```

**A/Bæµ‹è¯•ç¤ºä¾‹**ï¼š

| å‡è®¾ | å˜é‡ | Aç»„ï¼ˆå¯¹ç…§ï¼‰ | Bç»„ï¼ˆå®éªŒï¼‰ | ç»“æœ |
|------|------|------------|-------------|------|
| æ ‡é¢˜åŠ æ•°å­—æå‡ç‚¹å‡»ç‡ | æ ‡é¢˜æ ¼å¼ | "å¦‚ä½•ä¼˜åŒ–SQLæŸ¥è¯¢" | "ä¼˜åŒ–SQLæŸ¥è¯¢çš„5ä¸ªæŠ€å·§" | Bç»„ç‚¹å‡»ç‡+23% âœ… |
| é•¿æ–‡ç« æ›´å—æ¬¢è¿ | æ–‡ç« é•¿åº¦ | 2000å­— | 4000å­— | æ— æ˜¾è‘—å·®å¼‚ âŒ |
| é¦–æ®µåŠ é—®é¢˜å¸å¼•è¯»è€… | å¼€å¤´æ–¹å¼ | ç›´æ¥è®²è§£ | æå‡ºé—®é¢˜åœºæ™¯ | Bç»„åœç•™æ—¶é—´+15% âœ… |

é€šè¿‡æŒç»­çš„æ•°æ®åˆ†æå’ŒA/Bæµ‹è¯•ï¼Œä½ çš„å†…å®¹ç³»ç»Ÿä¼šè¶Šæ¥è¶Šäº†è§£å—ä¼—åå¥½ï¼Œäº§å‡ºè´¨é‡ä¸æ–­æå‡ã€‚

---

## æœ¬ç« å°ç»“

å†…å®¹ç”Ÿäº§è‡ªåŠ¨åŒ–ä¸æ˜¯"è®©AIå†™æ–‡ç« "è¿™ä¹ˆç®€å•ï¼Œè€Œæ˜¯æ„å»ºä¸€å¥—å®Œæ•´çš„ç³»ç»Ÿï¼š

**æ ¸å¿ƒç†å¿µ**ï¼š
- **åˆ›æ„å†³ç­–ç”±äººï¼Œæ‰§è¡ŒåŠ¨ä½œç”±AI**
- **PipelineåŒ–æ€ç»´**ï¼šå°†å¤æ‚æµç¨‹æ‹†åˆ†ä¸ºæ¸…æ™°çš„é˜¶æ®µ
- **å¿«é€ŸéªŒè¯**ï¼šä»ç—›ç‚¹åˆ°MVPåªéœ€è¦å‡ å°æ—¶ï¼Œè€Œä¸æ˜¯å‡ ä¸ªæœˆ

**ä¸‰å¤§Pipeline**ï¼š
1. **YouTubeå†…å®¹ç®¡é“**ï¼šé€‰é¢˜ä¾¦å¯Ÿ â†’ è„šæœ¬ç”Ÿæˆ â†’ å‘å¸ƒåˆ†æ
2. **å†…å®¹å·¥å‚**ï¼šå¤šAgentå¹¶è¡Œåä½œï¼Œå¤§è§„æ¨¡äº§å‡º
3. **åšå®¢è‡ªåŠ¨åŒ–**ï¼šä»å†™ä½œåˆ°SEOä¼˜åŒ–ä¸€é”®å®Œæˆ

**äº§å“éªŒè¯å¾ªç¯**ï¼š
- **Last 30 Days Skill**ï¼šä»Reddit/TwitteræŒ–æ˜çœŸå®ç—›ç‚¹
- **Overnight App Builder**ï¼šAIè‡ªåŠ¨æ„å»ºMVP
- **å¿«é€Ÿè¿­ä»£**ï¼šæ”¶é›†åé¦ˆ â†’ è°ƒæ•´æ–¹å‘ â†’ å†æ¬¡éªŒè¯

**å…³é”®æˆåŠŸå› ç´ **ï¼š
1. **äººå·¥ä»‹å…¥ç‚¹è®¾è®¡**ï¼šåœ¨å…³é”®ç¯èŠ‚ä¿ç•™äººå·¥å®¡æ ¸
2. **è´¨é‡æ§åˆ¶æœºåˆ¶**ï¼šè‡ªåŠ¨åŒ–æ£€æŸ¥ + æŠ½æ ·å®¡æ ¸
3. **æ•°æ®é©±åŠ¨ä¼˜åŒ–**ï¼šä»å·²å‘å¸ƒå†…å®¹çš„è¡¨ç°ä¸­å­¦ä¹ 

ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†æ¢è®¨**ç”Ÿäº§åŠ›ä¸é¡¹ç›®ç®¡ç†**ï¼šå¦‚ä½•è®©AI Agentæˆä¸ºä½ çš„ä¸ªäººåŠ©ç†å’Œé¡¹ç›®ç»ç†ï¼Œç®¡ç†å¤æ‚çš„å¤šä»»åŠ¡å·¥ä½œæµã€‚

---

**æ€è€ƒé¢˜**ï¼š
1. ä½ å½“å‰çš„å†…å®¹åˆ›ä½œæµç¨‹ä¸­ï¼Œå“ªä¸ªç¯èŠ‚æœ€è€—æ—¶ï¼Ÿèƒ½å¦ç”¨æœ¬ç« çš„æ–¹æ³•è‡ªåŠ¨åŒ–ï¼Ÿ
2. å¦‚æœè®©ä½ è®¾è®¡ä¸€ä¸ªå†…å®¹å·¥å‚ï¼Œä½ ä¼šåˆ†é…å“ªäº›Agentè§’è‰²ï¼Ÿ
3. ä½ çš„é¢†åŸŸæœ€å¤§çš„ç”¨æˆ·ç—›ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿå°è¯•ç”¨Last 30 Days SkillæŒ–æ˜ä¸€ä¸‹ã€‚

**ç»ƒä¹ **ï¼š
é€‰æ‹©ä¸€ä¸ªç®€å•çš„å†…å®¹é¡¹ç›®ï¼ˆå¦‚ä¸ªäººåšå®¢ã€ç¤¾äº¤åª’ä½“è´¦å·ï¼‰ï¼Œå®ç°ä¸€ä¸ªåŸºç¡€çš„è‡ªåŠ¨åŒ–Pipelineï¼Œè‡³å°‘åŒ…å«ï¼šé€‰é¢˜ â†’ ç ”ç©¶ â†’ èµ·è‰ä¸‰ä¸ªé˜¶æ®µã€‚è¿è¡Œä¸€å‘¨ï¼Œè®°å½•æ—¶é—´èŠ‚çœå’Œè´¨é‡å˜åŒ–ã€‚

---

## å‚è€ƒèµ„æ–™

æœ¬ç« å¼•ç”¨çš„æ¡ˆä¾‹å‡æ¥è‡ª [awesome-openclaw-usecases](https://github.com/hesamsheikh/awesome-openclaw-usecases) ç¤¾åŒºä»“åº“ï¼š

- [YouTube Content Pipeline](https://github.com/hesamsheikh/awesome-openclaw-usecases/blob/main/usecases/youtube-content-pipeline.md)
- [Multi-Agent Content Factory](https://github.com/hesamsheikh/awesome-openclaw-usecases/blob/main/usecases/content-factory.md)
- [Goal-Driven Autonomous Tasks](https://github.com/hesamsheikh/awesome-openclaw-usecases/blob/main/usecases/overnight-mini-app-builder.md)
